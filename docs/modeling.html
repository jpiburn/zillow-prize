<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Zillow Prize Modeling</title>
  <meta name="description" content="This is to document an example workflow and predictive modeling process using the Kaggle Zillow competition as an example.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Zillow Prize Modeling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is to document an example workflow and predictive modeling process using the Kaggle Zillow competition as an example." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Zillow Prize Modeling" />
  
  <meta name="twitter:description" content="This is to document an example workflow and predictive modeling process using the Kaggle Zillow competition as an example." />
  

<meta name="author" content="Jesse Piburn">


<meta name="date" content="2018-07-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="feature-selection.html">
<link rel="next" href="summary.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>
<link href="libs/dt-ext-fixedcolumns-1.10.12/css/fixedColumns.dataTables.min.css" rel="stylesheet" />
<script src="libs/dt-ext-fixedcolumns-1.10.12/js/dataTables.fixedColumns.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Predictive Modeling Workflow</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#problem"><i class="fa fa-check"></i><b>1.1</b> Problem</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#evaluation"><i class="fa fa-check"></i><b>1.2</b> Evaluation</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#initital-thoughts"><i class="fa fa-check"></i><b>1.3</b> Initital Thoughts</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#note-on-using-external-features"><i class="fa fa-check"></i><b>1.4</b> Note on Using External Features</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="preprocessing.html"><a href="preprocessing.html"><i class="fa fa-check"></i><b>2</b> PreProcessing</a><ul>
<li class="chapter" data-level="2.1" data-path="preprocessing.html"><a href="preprocessing.html#the-raw-data"><i class="fa fa-check"></i><b>2.1</b> The Raw Data</a><ul>
<li class="chapter" data-level="2.1.1" data-path="preprocessing.html"><a href="preprocessing.html#saving-raw-data-using-feather"><i class="fa fa-check"></i><b>2.1.1</b> Saving Raw Data Using <code>feather</code></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="preprocessing.html"><a href="preprocessing.html#renaming-variables"><i class="fa fa-check"></i><b>2.2</b> Renaming Variables</a><ul>
<li class="chapter" data-level="2.2.1" data-path="preprocessing.html"><a href="preprocessing.html#renaming-properties-features"><i class="fa fa-check"></i><b>2.2.1</b> Renaming <code>properties</code> Features</a></li>
<li class="chapter" data-level="2.2.2" data-path="preprocessing.html"><a href="preprocessing.html#renaming-train-features"><i class="fa fa-check"></i><b>2.2.2</b> renaming <code>train</code> features</a></li>
<li class="chapter" data-level="2.2.3" data-path="preprocessing.html"><a href="preprocessing.html#basic-transformations"><i class="fa fa-check"></i><b>2.2.3</b> Basic Transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="preprocessing.html"><a href="preprocessing.html#extracting-geographic-information"><i class="fa fa-check"></i><b>2.3</b> Extracting Geographic Information</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>3</b> Exploratory Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="eda.html"><a href="eda.html#response-variable"><i class="fa fa-check"></i><b>3.1</b> Response Variable</a><ul>
<li class="chapter" data-level="3.1.1" data-path="eda.html"><a href="eda.html#transactions-over-time"><i class="fa fa-check"></i><b>3.1.1</b> Transactions Over Time</a></li>
<li class="chapter" data-level="3.1.2" data-path="eda.html"><a href="eda.html#spatial-distribution-of-log_error"><i class="fa fa-check"></i><b>3.1.2</b> Spatial Distribution of <code>log_error</code></a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="eda.html"><a href="eda.html#predictor-variables"><i class="fa fa-check"></i><b>3.2</b> Predictor Variables</a><ul>
<li class="chapter" data-level="3.2.1" data-path="eda.html"><a href="eda.html#missingness"><i class="fa fa-check"></i><b>3.2.1</b> Missingness</a></li>
<li class="chapter" data-level="3.2.2" data-path="eda.html"><a href="eda.html#numeric-features"><i class="fa fa-check"></i><b>3.2.2</b> Numeric Features</a></li>
<li class="chapter" data-level="3.2.3" data-path="eda.html"><a href="eda.html#numeric-outliers"><i class="fa fa-check"></i><b>3.2.3</b> Numeric Outliers</a></li>
<li class="chapter" data-level="3.2.4" data-path="eda.html"><a href="eda.html#categorical-features"><i class="fa fa-check"></i><b>3.2.4</b> Categorical Features</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="eda.html"><a href="eda.html#exploring-log_error-a-little-more"><i class="fa fa-check"></i><b>3.3</b> Exploring <code>log_error</code> A little More</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="feat-eng.html"><a href="feat-eng.html"><i class="fa fa-check"></i><b>4</b> Feature Engineering</a><ul>
<li class="chapter" data-level="4.1" data-path="feat-eng.html"><a href="feat-eng.html#creating-new-features"><i class="fa fa-check"></i><b>4.1</b> Creating New Features</a><ul>
<li class="chapter" data-level="4.1.1" data-path="feat-eng.html"><a href="feat-eng.html#internal-features"><i class="fa fa-check"></i><b>4.1.1</b> Internal Features</a></li>
<li class="chapter" data-level="4.1.2" data-path="feat-eng.html"><a href="feat-eng.html#external-features"><i class="fa fa-check"></i><b>4.1.2</b> External Features</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="feat-eng.html"><a href="feat-eng.html#handling-missing-data"><i class="fa fa-check"></i><b>4.2</b> Handling Missing Data</a></li>
<li class="chapter" data-level="4.3" data-path="feat-eng.html"><a href="feat-eng.html#feature-transformation"><i class="fa fa-check"></i><b>4.3</b> Feature Transformation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="feature-selection.html"><a href="feature-selection.html"><i class="fa fa-check"></i><b>5</b> Feature Selection</a></li>
<li class="chapter" data-level="6" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>6</b> Modeling</a><ul>
<li class="chapter" data-level="6.1" data-path="modeling.html"><a href="modeling.html#xgboost"><i class="fa fa-check"></i><b>6.1</b> XGBoost</a></li>
<li class="chapter" data-level="6.2" data-path="modeling.html"><a href="modeling.html#our-recipe-for-success"><i class="fa fa-check"></i><b>6.2</b> Our Recipe for Success</a></li>
<li class="chapter" data-level="6.3" data-path="modeling.html"><a href="modeling.html#base-line-model"><i class="fa fa-check"></i><b>6.3</b> Base Line Model</a><ul>
<li class="chapter" data-level="6.3.1" data-path="modeling.html"><a href="modeling.html#making-predictions-with-base-line-model"><i class="fa fa-check"></i><b>6.3.1</b> Making Predictions with Base Line Model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="modeling.html"><a href="modeling.html#hyperparameter-optimization"><i class="fa fa-check"></i><b>6.4</b> Hyperparameter Optimization</a><ul>
<li class="chapter" data-level="6.4.1" data-path="modeling.html"><a href="modeling.html#creating-our-scoring-function"><i class="fa fa-check"></i><b>6.4.1</b> Creating Our Scoring Function</a></li>
<li class="chapter" data-level="6.4.2" data-path="modeling.html"><a href="modeling.html#parameter-search-space"><i class="fa fa-check"></i><b>6.4.2</b> Parameter Search Space</a></li>
<li class="chapter" data-level="6.4.3" data-path="modeling.html"><a href="modeling.html#fold-cross-validation"><i class="fa fa-check"></i><b>6.4.3</b> 5-fold Cross Validation</a></li>
<li class="chapter" data-level="6.4.4" data-path="modeling.html"><a href="modeling.html#random-forest-search-of-parameter-space"><i class="fa fa-check"></i><b>6.4.4</b> Random Forest Search of Parameter Space</a></li>
<li class="chapter" data-level="6.4.5" data-path="modeling.html"><a href="modeling.html#exploring-parameter-space"><i class="fa fa-check"></i><b>6.4.5</b> Exploring Parameter Space</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="modeling.html"><a href="modeling.html#tuned-model"><i class="fa fa-check"></i><b>6.5</b> Tuned Model</a><ul>
<li class="chapter" data-level="6.5.1" data-path="modeling.html"><a href="modeling.html#making-predictions-with-tuned-model"><i class="fa fa-check"></i><b>6.5.1</b> Making Predictions with Tuned Model</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="modeling.html"><a href="modeling.html#model-comparison"><i class="fa fa-check"></i><b>6.6</b> Model Comparison</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>7</b> Summary</a><ul>
<li class="chapter" data-level="7.1" data-path="summary.html"><a href="summary.html#key-findings"><i class="fa fa-check"></i><b>7.1</b> Key Findings</a></li>
<li class="chapter" data-level="7.2" data-path="summary.html"><a href="summary.html#next-steps"><i class="fa fa-check"></i><b>7.2</b> Next Steps</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Zillow Prize Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modeling" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Modeling</h1>
<p>For our first pass a submission, we are going to use the XGBoost model. This model has seen much success in Kaggle competitions and wide use across industry and academia due to its flexiblity, speed, and range of modeling tasks it can be applied to. We’ll start by creating a base line model and then try our hand at tuning a few of the XGBoost parameters to see how of performance changes.</p>
<div id="xgboost" class="section level2">
<h2><span class="header-section-number">6.1</span> XGBoost</h2>
<p>XGBoost, which stands for eXtreme Gradient Boosting, is a highly optimized and flexible implementation of gradient boosted decision trees. At a high level boosted trees work by creating ensembles of decision trees and uses gradient boosting (additive training) to combine each of the tree’s predictions into one strong prediction by optimizing over any differentiable loss function. In practice a regularization penalty is added to the loss function to help control complexity.</p>
<p>Decision trees, particularly XGBoost is widely used for several reasons such as its flexibilty to be used for numerous problem types across both regression and classification, it’s invariance to scaling inputs, and it’s ability to scale to handle large amounts of data.</p>
<p>For more information about XGBoost see Chen &amp; Guestrin<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> and Gradient Boosting in general, Fridmean (1999)<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
</div>
<div id="our-recipe-for-success" class="section level2">
<h2><span class="header-section-number">6.2</span> Our Recipe for Success</h2>
<p>As a reminder our recipe for our transformations are stored in <code>rec</code> that we created in Chapter 4. It’s looks like this</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rec</code></pre></div>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         98
## 
## Operations:
## 
## Mean Imputation for starts_with(&quot;roll_&quot;)
## Zero variance filter on all_numeric()
## Box-Cox transformation on 6 items
## Dummy variables from all_nominal()
## Interactions with starts_with(&quot;tax_&quot;):starts_with(&quot;area_&quot;)
## Centering for all_numeric(), -all_outcomes()
## Scaling for all_numeric(), -all_outcomes()
## PCA extraction with starts_with(&quot;acs_home_value_cnt&quot;)
## Zero variance filter on all_numeric()</code></pre>
</div>
<div id="base-line-model" class="section level2">
<h2><span class="header-section-number">6.3</span> Base Line Model</h2>
<blockquote>
<p>Alice: “Would you tell me, please, which way I ought to go from here?”</p>
<p>The Cheshire Cat: “That depends a good deal on where you want to get to.”</p>
<p>Alice: “I don’t much care where.”</p>
<p>The Cheshire Cat: “Then it doesn’t matter which way you go.”</p>
<p>— The Adventures of Alice in Wonderland, on the need for base line models in machine learning (that’s my interpretation at least)</p>
</blockquote>
<p>Making Predictive models is an iterative process, a process in which it is easy to get pulled in wrong directions by needless complexity, frtuitless tweaks, and soul crushingly poor performing ideas you thought might actually work.</p>
<p>To help tether ourselves to reality, it is useful to generate a base line model to compare all other models against before we get into any of the more complex tasks like parameter tuning. This will help guide what direction we should take next so we don’t end up like Alice.</p>
<p>Kaggle competitions have done wonders for our collective obession for improving predictive accuracy with wild disregard for the time and effort it took to achieve that improvement. In a typical business or scientific academic application, improvement in predictive accuracy has to be balanced with the cost of acheiving it. A base line model can help use measure these diminishing returns.</p>
<p>For our base line model, we will use the top <code>20</code> features returned from our feature selection process in an xgboost model with default parameter settings and with <code>nrounds</code>, the maximum number of boosting iterations, set to <code>1000</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">features_to_use_baseline &lt;-<span class="st"> </span>feature_avg <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(mean)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">head</span>(<span class="dv">20</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>.<span class="op">$</span>Feature

d_prepped &lt;-<span class="st"> </span><span class="kw">prep</span>(rec)

train_df &lt;-<span class="st"> </span><span class="kw">bake</span>(d_prepped, <span class="dt">newdata =</span> d)

x_train &lt;-<span class="st"> </span>train_df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(features_to_use_baseline) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as.matrix</span>()

y_train &lt;-<span class="st"> </span>train_df<span class="op">$</span>log_error

xgb_train_data &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(x_train, <span class="dt">label =</span> y_train)

base_model &lt;-<span class="st"> </span><span class="kw">xgb.train</span>(<span class="dt">data =</span> xgb_train_data,
                        <span class="dt">objective =</span> <span class="st">&#39;reg:linear&#39;</span>,
                        <span class="dt">verbose =</span> <span class="ot">FALSE</span>,
                        <span class="dt">nrounds =</span> <span class="dv">1000</span>,
                        <span class="dt">nthread =</span> <span class="dv">20</span>)</code></pre></div>
<div id="making-predictions-with-base-line-model" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Making Predictions with Base Line Model</h3>
<p>Now we finally get to make our first predictions! Since we’ll be doing this again and potentially many times, let’s make a helper function <code>predict_date()</code> to do this. This function will take, the parcels (<code>parcel_id</code>) and the dates (<code>predict_date</code>) we wish to predict, along with the model (<code>mdl</code>) and features we want to use to (<code>features_to_use</code>) to do the predictions</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">predict_date &lt;-<span class="st"> </span><span class="cf">function</span>(parcel_id, predict_date, mdl, features_to_use) {

d_predict_ids &lt;-<span class="st"> </span>properties <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(id_parcel <span class="op">%in%</span><span class="st"> </span>parcel_id) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">crossing</span>(<span class="dt">date =</span> predict_date)

d_predict &lt;-<span class="st"> </span>d_predict_ids <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">year =</span> <span class="kw">year</span>(date),
    <span class="dt">month_year =</span> <span class="kw">make_date</span>(<span class="kw">year</span>(date), <span class="kw">month</span>(date)),
    <span class="dt">month =</span> <span class="kw">month</span>(date, <span class="dt">label =</span> <span class="ot">TRUE</span>),
    <span class="dt">week =</span> <span class="kw">floor_date</span>(date, <span class="dt">unit =</span> <span class="st">&quot;week&quot;</span>),
    <span class="dt">week_of_year =</span> <span class="kw">week</span>(date),
    <span class="dt">week_since_start =</span> (<span class="kw">min</span>(date) <span class="op">%--%</span><span class="st"> </span>date <span class="op">%/%</span><span class="st"> </span><span class="kw">dweeks</span>()) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>,
    <span class="dt">wday =</span> <span class="kw">wday</span>(date, <span class="dt">label =</span> <span class="ot">TRUE</span>),
    <span class="dt">day_of_month =</span> <span class="kw">day</span>(date)
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(econ_features, <span class="dt">by =</span> <span class="st">&quot;date&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(roll_bg, <span class="dt">by =</span> <span class="kw">c</span>(<span class="st">&quot;id_geo_bg_fips&quot;</span>, <span class="st">&quot;date&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(roll_tract, <span class="dt">by =</span> <span class="kw">c</span>(<span class="st">&quot;id_geo_tract_fips&quot;</span>, <span class="st">&quot;date&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(
    <span class="op">-</span>id_parcel,
    <span class="op">-</span>week,
    <span class="op">-</span>region_city,
    <span class="op">-</span>region_zip,
    <span class="op">-</span>area_lot_jitter,
    <span class="op">-</span>area_lot_quantile,
    <span class="op">-</span>lat, <span class="co"># we have other lat/lon features</span>
    <span class="op">-</span>lon, 
    <span class="op">-</span><span class="kw">starts_with</span>(<span class="st">&quot;id_geo&quot;</span>)
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">date =</span> <span class="kw">as.numeric</span>(date),
    <span class="dt">year =</span> <span class="kw">factor</span>(year, <span class="dt">levels =</span> <span class="kw">sort</span>(<span class="kw">unique</span>(year)), <span class="dt">ordered =</span> <span class="ot">TRUE</span>),
    <span class="dt">month_year =</span> <span class="kw">factor</span>(
      <span class="kw">as.character</span>(month_year), 
      <span class="dt">levels =</span> <span class="kw">as.character</span>(<span class="kw">unique</span>(<span class="kw">sort</span>(month_year))), 
      <span class="dt">ordered =</span> <span class="ot">TRUE</span>),
    <span class="dt">str_quality =</span> <span class="kw">factor</span>(str_quality, <span class="dt">levels =</span> <span class="dv">12</span><span class="op">:</span><span class="dv">1</span>, <span class="dt">ordered =</span> <span class="ot">TRUE</span>)
  )

eval_df &lt;-<span class="st"> </span><span class="kw">bake</span>(d_prepped, <span class="dt">newdata =</span> d_predict)

x_eval &lt;-<span class="st"> </span>eval_df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(features_to_use) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">as.matrix</span>()

preds &lt;-<span class="st"> </span><span class="kw">predict</span>(mdl, x_eval)

properties_predict &lt;-<span class="st"> </span>d_predict_ids <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(
    id_parcel,
    date
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">pred =</span> preds
    ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread</span>(date, pred)

<span class="kw">names</span>(properties_predict) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;ParcelId&quot;</span>, <span class="st">&quot;201610&quot;</span>, <span class="st">&quot;201611&quot;</span>, 
                               <span class="st">&quot;201612&quot;</span>, <span class="st">&quot;201710&quot;</span>, <span class="st">&quot;201711&quot;</span>, <span class="st">&quot;201712&quot;</span>)

properties_predict

}</code></pre></div>
<p>The submission requires a prediction for Oct-Dec 2016 and Oct-Dec 2017. This means that the prediction is for any day in that month. For our example submissions, we are going to set the date to the first Wednesday in each month. This is completely arbitrary, but provides a simple starting point. Another approach would be to make predictions for every day in each month and submit the mean prediction for each month. We’ll save this for later work.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># first wednesday in each month</span>
predict_dates &lt;-<span class="st"> </span><span class="kw">date</span>(<span class="kw">c</span>(<span class="st">&quot;2016-10-06&quot;</span>, <span class="st">&quot;2016-11-02&quot;</span>, <span class="st">&quot;2016-12-07&quot;</span>, 
                        <span class="st">&quot;2017-10-04&quot;</span>, <span class="st">&quot;2017-11-01&quot;</span>, <span class="st">&quot;2017-12-06&quot;</span>))

<span class="co"># split parcels to chunck our predictions</span>
id_parcels &lt;-<span class="st"> </span>properties<span class="op">$</span>id_parcel
id_parcel_splits &lt;-<span class="st"> </span><span class="kw">split</span>(id_parcels, <span class="kw">ceiling</span>(<span class="kw">seq_along</span>(id_parcels) <span class="op">/</span><span class="st"> </span><span class="dv">5000</span>))</code></pre></div>
<p>Now make the predictions with <code>base_model</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">base_predict_list &lt;-<span class="st"> </span><span class="kw">lapply</span>(id_parcel_splits, <span class="cf">function</span>(i) {
  
  pred_df &lt;-<span class="st"> </span><span class="kw">predict_date</span>(
    <span class="dt">parcel_id =</span> i, 
    <span class="dt">predict_date =</span> predict_dates, 
    <span class="dt">mdl =</span> base_model,
    <span class="dt">features_to_use =</span> features_to_use_baseline
    )
  })

<span class="co"># they only evaluate to 4 decimcals so round to save space</span>
<span class="co"># Convert ParcelId to integer to prevent Sci Notation that causes</span>
<span class="co"># issues with submission</span>
base_predict_df &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(base_predict_list) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate_at</span>(<span class="kw">vars</span>(<span class="st">`</span><span class="dt">201610</span><span class="st">`</span><span class="op">:</span><span class="st">`</span><span class="dt">201712</span><span class="st">`</span>), round, <span class="dt">digits =</span> <span class="dv">4</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">ParcelId =</span> <span class="kw">as.integer</span>(ParcelId)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as.data.frame</span>()

<span class="kw">write_csv</span>(base_predict_df, <span class="st">&quot;submissions/base_submit.csv&quot;</span>)</code></pre></div>
<p>The resulting MAE of this submission was <code>0.0789722</code> on the public leaderboard. Not that great, but hey at least we have room to improve.</p>
<p>Save this value for comparison</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">base_mae &lt;-<span class="st"> </span><span class="fl">0.0789722</span></code></pre></div>
<p>Let’s see if tuning the parameters of the XGBoost model can help improve that.</p>
</div>
</div>
<div id="hyperparameter-optimization" class="section level2">
<h2><span class="header-section-number">6.4</span> Hyperparameter Optimization</h2>
<p>The double edged sword of the XGBoost model is how widely varying the predictions it generates can be depending on what the models hyperparameters are set to. The good is that it allows us to be very flexible across many different datasets, the bad is it almost always requires us to perform some type of hyperparameter optimization to fine tune its’ performance. The ugly is the code needed to perform this.</p>
<p>Our parameter optimization strategy is as follows</p>
<ol style="list-style-type: decimal">
<li><p>Create a scoring function to capture model performance using MAE</p></li>
<li><p>Use 5-fold cross validation to resample our training data</p></li>
<li><p>Randomly generate intital parameter combinations.</p></li>
<li><p>Using step 3 as an intital input, use a surrogate Random Forest model to progressively zoom into parameter space based on the average performance of parameter sets across the resampled data on our scoring function. We will set this to 4 runs for now, zooming further down each time.</p></li>
</ol>
<div id="creating-our-scoring-function" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Creating Our Scoring Function</h3>
<p>To assess our parameter tuning performance we will create the function <code>xgboost_regress_score()</code> that will capture the MAE of each resample and parameter combination of our data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create scoring function -------------------------------------------------</span>

xgboost_regress_score &lt;-<span class="st"> </span><span class="cf">function</span>(train_df, target_var, params, eval_df, ...) {
    
    X_train &lt;-<span class="st"> </span>train_df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">select</span>(features_to_use) <span class="op">%&gt;%</span>
<span class="st">      </span><span class="kw">as.matrix</span>()
    
    y_train &lt;-<span class="st"> </span>train_df[[target_var]]
    xgb_train_data &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(X_train, <span class="dt">label =</span> y_train)
    
    X_eval &lt;-<span class="st"> </span>eval_df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">select</span>(features_to_use) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">as.matrix</span>()
    
    y_eval &lt;-<span class="st"> </span>eval_df[[target_var]]
    
    xgb_eval_data &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(X_eval, <span class="dt">label =</span> y_eval)
    
    model &lt;-<span class="st"> </span><span class="kw">xgb.train</span>(<span class="dt">params =</span> params,
                       <span class="dt">data =</span> xgb_train_data,
                       <span class="dt">watchlist =</span> <span class="kw">list</span>(<span class="dt">train =</span> xgb_train_data, <span class="dt">eval =</span> xgb_eval_data),
                       <span class="dt">objective =</span> <span class="st">&#39;reg:linear&#39;</span>,
                       <span class="dt">verbose =</span> <span class="ot">FALSE</span>,
                       <span class="dt">nthread =</span> <span class="dv">20</span>,
                       ...)
    
    preds &lt;-<span class="st"> </span><span class="kw">predict</span>(model, xgb_eval_data)
    
    <span class="kw">list</span>(<span class="dt">mae =</span> <span class="kw">MAE</span>(preds, y_eval))
    
  }</code></pre></div>
</div>
<div id="parameter-search-space" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Parameter Search Space</h3>
<p>We will use the following parameters</p>
<ul>
<li><code>eta</code> [default=0.3, alias: <code>learning_rate</code>]
<ul>
<li>Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.</li>
<li>range: [0, 1]</li>
</ul></li>
<li><code>gamma</code> [default=0, alias: <code>min_split_loss</code>]
<ul>
<li>Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger <code>gamma</code> is, the more conservative the algorithm will be.</li>
<li>range[0, <code>Inf</code>]</li>
</ul></li>
<li><code>max_depth</code> [default=6]
<ul>
<li>Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 indicates no limit.</li>
<li>range: [0, <code>Inf</code>]</li>
</ul></li>
<li><code>min_child_weight</code> [default=1]
<ul>
<li>Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than <code>min_child_weight</code>, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger <code>min_child_weight</code> is, the more conservative the algorithm will be.</li>
<li>range: [0, <code>Inf</code>]</li>
</ul></li>
<li><code>subsample</code> [default=1]
<ul>
<li>Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.</li>
<li>range: (0, 1]</li>
</ul></li>
<li><code>colsample_bytree</code> [default=1]
<ul>
<li>Subsample ratio of columns when constructing each tree. Subsampling will occur once in every boosting iteration.</li>
<li>range: (0, 1]</li>
</ul></li>
</ul>
<p>Based on the performance of these optimizations other parameters could be included later. I set the ranges on these parameters with the idea that our base line model might be overfiting.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ParamHelpers)
<span class="co"># make parameter set ------------------------------------------------------</span>
xgboost_random_params &lt;-
<span class="st">  </span><span class="kw">makeParamSet</span>(
    <span class="kw">makeIntegerParam</span>(<span class="st">&#39;max_depth&#39;</span>, <span class="dt">lower =</span> <span class="dv">1</span>, <span class="dt">upper =</span> <span class="dv">15</span>),
    <span class="kw">makeNumericParam</span>(<span class="st">&#39;eta&#39;</span>, <span class="dt">lower =</span> <span class="fl">0.01</span>, <span class="dt">upper =</span> <span class="fl">0.1</span>),
    <span class="kw">makeNumericParam</span>(<span class="st">&#39;gamma&#39;</span>, <span class="dt">lower =</span> <span class="dv">0</span>, <span class="dt">upper =</span> <span class="dv">5</span>),
    <span class="kw">makeIntegerParam</span>(<span class="st">&#39;min_child_weight&#39;</span>, <span class="dt">lower =</span> <span class="dv">1</span>, <span class="dt">upper =</span> <span class="dv">100</span>),
    <span class="kw">makeNumericParam</span>(<span class="st">&#39;subsample&#39;</span>, <span class="dt">lower =</span> <span class="fl">0.25</span>, <span class="dt">upper =</span> <span class="fl">0.9</span>),
    <span class="kw">makeNumericParam</span>(<span class="st">&#39;colsample_bytree&#39;</span>, <span class="dt">lower =</span> <span class="fl">0.25</span>, <span class="dt">upper =</span> <span class="fl">0.9</span>)
  )</code></pre></div>
</div>
<div id="fold-cross-validation" class="section level3">
<h3><span class="header-section-number">6.4.3</span> 5-fold Cross Validation</h3>
<p>Set up our resampling via 5 fold cross validation</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create cv resampling ----------------------------------------------------</span>
resamples &lt;-<span class="st"> </span><span class="kw">vfold_cv</span>(d, <span class="dt">v =</span> <span class="dv">5</span>) </code></pre></div>
</div>
<div id="random-forest-search-of-parameter-space" class="section level3">
<h3><span class="header-section-number">6.4.4</span> Random Forest Search of Parameter Space</h3>
<p>All of the heavy lifting for the surrogate Random Forest model is contained in the <code>tidytune::surrogate_search()</code> function.</p>
<p>The progressive zoom into more promising parameter space is done by passing a vector to <code>n</code> and <code>n_canidates</code>. Higher values for <code>n_candidates</code> will restrict the search to better performing areas and lower values increase the randnomness (with <code>0</code> falling back on random search).<code>n</code> is the number of runs of the surrogate model that will run for each of our 5 folds.</p>
<p><code>top_n</code> is set to 5, which tells the model out of the top <code>n_candidates</code> pass along the <code>top_n</code> to the next round.</p>
<p>We will do for runs of the surrogate Random Forest model, zooming into more performant parameter space each time.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MLmetrics)
<span class="kw">library</span>(xgboost)
<span class="kw">library</span>(tidytune)

<span class="co"># perform surrogate search over parameters --------------------------------</span>
n &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">2</span>)
<span class="co"># start with random and then zoom in</span>
n_candidates &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>)

search_results &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">surrogate_search</span>(
    <span class="dt">resamples =</span> resamples,
    <span class="dt">recipe =</span> rec,
    <span class="dt">param_set =</span> xgboost_random_params,
    <span class="dt">n =</span> n,
    <span class="dt">scoring_func =</span> xgboost_regress_score,
    <span class="dt">nrounds =</span> <span class="dv">1000</span>,
    <span class="dt">early_stopping_rounds =</span> <span class="dv">20</span>,
    <span class="dt">eval_metric =</span> <span class="st">&#39;mae&#39;</span>,
    <span class="dt">input =</span> <span class="ot">NULL</span>,
    <span class="dt">surrogate_target =</span> <span class="st">&#39;mae&#39;</span>,
    <span class="dt">n_candidates =</span> n_candidates,
    <span class="dt">top_n =</span> <span class="dv">5</span>
  )</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">search_summary &lt;-<span class="st"> </span>
<span class="st">  </span>search_results <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by_at</span>(<span class="kw">getParamIds</span>(xgboost_random_params)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">mae =</span> <span class="kw">mean</span>(mae)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(mae)</code></pre></div>
<div id="htmlwidget-13b64cb40d0c007cf4f5" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-13b64cb40d0c007cf4f5">{"x":{"filter":"none","extensions":["FixedColumns"],"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100"],[5,5,5,5,8,6,6,5,5,3,6,3,6,7,14,5,8,6,13,9,4,9,10,10,7,10,8,7,9,11,7,5,4,7,3,11,15,14,2,6,4,9,2,9,2,9,7,14,5,13,7,13,3,12,2,4,5,10,3,15,12,15,12,3,10,3,4,10,14,12,4,3,15,13,2,15,11,2,7,4,5,5,11,4,10,2,13,13,15,6,12,9,2,1,14,14,1,1,1,11],[0.0753141567087732,0.0254947707173415,0.0119649214693345,0.0227248513093218,0.0179595448542386,0.0184877650788985,0.0597224851348437,0.0427106887870468,0.0147431414970197,0.0400677399151027,0.0331406971719116,0.0784377305721864,0.0732688342011534,0.0376480637793429,0.032756173487287,0.0912110299943015,0.0190379928704351,0.0750838153879158,0.087546484915074,0.0425527004734613,0.0916547728097066,0.0493282221909613,0.057776785925962,0.0162993016466498,0.0628440136881545,0.0884419636335224,0.0783674281137064,0.0353907304187305,0.0225892958813347,0.0470733283623122,0.0137403579684906,0.0449148067017086,0.0710218821209855,0.093172436428722,0.0338219041447155,0.0773266969644465,0.0881646320014261,0.0803001537709497,0.0430891525233165,0.0359831988741644,0.086077690012753,0.0926527520827949,0.0432295897603035,0.0942528136866167,0.0311913295811974,0.0944529314781539,0.0223123435303569,0.0329659490543418,0.0419761153473519,0.0362351771770045,0.0493529334454797,0.0121952671487816,0.0275447937124409,0.0657443933538161,0.0741396811255254,0.0440491563058458,0.0263638722780161,0.0551452469825745,0.0758791842171922,0.0796454007341526,0.0174016687227413,0.0548089141072705,0.0569135553017259,0.0679850445897318,0.0841749036544934,0.0118694639136083,0.0759735290543176,0.0200919728539884,0.0692503520078026,0.0427176694665104,0.0535356798185967,0.0937310382025316,0.094472052778583,0.0276902329130098,0.0734936088277027,0.0984559041331522,0.0535413559176959,0.0400841548666358,0.0927033046400175,0.0836953385756351,0.0294213125482202,0.0836837697471492,0.0836468037520535,0.0834025090560317,0.0228970438684337,0.0201965126837604,0.0744446090632118,0.0882158134318888,0.0646870549721643,0.0151076148101129,0.0665221359208226,0.0395258455164731,0.0520585039770231,0.09136651806999,0.0442508784355596,0.0728647024580278,0.0556834023399279,0.0709851420810446,0.0195321366563439,0.0376399092772044],[2.00997113599442,2.04678243258968,2.05316238454543,1.79130512056872,2.05562060466036,2.47866926947609,2.54486419609748,2.11250258260407,2.22801433177665,1.93769115372561,2.33917575096712,2.14077465934679,1.96430495008826,1.92507048486732,2.27922895574011,2.33264146489091,2.09161106613465,2.74769272422418,3.01325934822671,2.14223331189714,2.04470272758044,1.64772397140041,1.85918637085706,2.22095836186782,4.54196820617653,3.02995549049228,2.38358426606283,2.73742980207317,1.92256364971399,1.93199952482246,1.08778516878374,2.51646730233915,2.77411713381298,2.12443486438133,2.67117578652687,2.18032279866748,4.98140327283181,1.88023202936165,1.78384115337394,1.30921784555539,2.85352528677322,4.23144670203328,2.03758930903859,2.9737269715406,2.00436749262735,3.76063668169081,2.50859185238369,1.85635066474788,2.03934338409454,2.91609084582888,3.29687153687701,1.52589268516749,0.542936158599332,4.60948667721823,2.54195492481813,1.49313916335814,2.63640755438246,2.01366083696485,3.97841057972983,4.96831585420296,4.99883497599512,1.83352405787446,4.18634631903842,3.51688755908981,1.0752314212732,1.91490195225924,0.478291240287945,4.73683991702273,3.11530667357147,3.61436476465315,4.77623535320163,0.664941967697814,1.51144838077016,1.98559789103456,3.54717011214234,1.18136492324993,1.43356147455052,3.10442587011494,4.74777650437318,3.55786583852023,4.98635521857068,3.41895746649243,1.43292709253728,4.24950017011724,2.89593482855707,1.92964088753797,3.25208902359009,3.48549937130883,1.26131552853622,4.76839752169326,0.547937586670741,0.875227507203817,3.93268856685609,0.446243697078899,0.94101645052433,0.736263532890007,3.91129626659676,4.25271159037948,3.52992220898159,0.144841232104227],[100,38,26,87,93,64,85,94,19,65,97,29,51,68,99,99,70,59,40,68,59,82,87,21,26,31,33,70,23,50,98,91,94,96,69,34,34,91,16,15,95,54,57,53,23,85,80,73,51,34,58,67,64,17,53,77,40,32,15,61,33,51,93,87,100,44,76,34,41,93,27,73,54,29,79,80,24,26,56,70,77,72,35,56,46,60,26,5,32,99,79,48,73,80,14,45,61,96,34,7],[0.882115679827984,0.842303937417455,0.866748715774156,0.890362443809863,0.891160581179429,0.83313264483586,0.858863256021868,0.745799078559503,0.798432543617673,0.837546187068801,0.787826500029769,0.847019029734656,0.83813094968209,0.821950826561078,0.83883860061178,0.781976540083997,0.793968101078644,0.855655375355855,0.870088608737569,0.667703014414292,0.821416241908446,0.87128560745623,0.836145404912531,0.895942453294992,0.869178457581438,0.879807806760073,0.828046001365874,0.663039878907148,0.828269104554784,0.843406377441715,0.713082389428746,0.595153071568348,0.804992544604465,0.633665808220394,0.841228108550422,0.871537832531612,0.792624737741426,0.844641920470167,0.881916406773962,0.680377196380869,0.821089530654717,0.738830135622993,0.79564038462704,0.593188817903865,0.7758536684094,0.809556493745185,0.439576119766571,0.431714642397128,0.424764922051691,0.482689452974591,0.48224827531958,0.471062022331171,0.60928943248,0.57167732482776,0.848401239304803,0.364641716016922,0.426429180952255,0.594363111245912,0.749801023281179,0.732595994300209,0.750119571713731,0.44667190265609,0.752937328489497,0.844942069600802,0.579611036891583,0.448234772589058,0.610233824595343,0.517067232704721,0.370813621068373,0.511878029652871,0.68806850835681,0.763388424669392,0.76243516902905,0.414411625207867,0.851678906439338,0.810083447536454,0.676026034343522,0.867030145518947,0.578316958155483,0.386401405651122,0.802844431251287,0.374889224441722,0.46486966565717,0.552576222782955,0.288196068769321,0.30072202993324,0.255329690384679,0.713541540061124,0.634949830418918,0.5883418336045,0.747994183562696,0.269919144548476,0.64592367541045,0.598801149358042,0.872551104740705,0.443961014109664,0.286688895709813,0.432570425292943,0.336253874609247,0.762967249308713],[0.528489873954095,0.55284611225361,0.553988792502787,0.436710486758966,0.521792834519874,0.812140957009979,0.88247439750703,0.638513505784795,0.84205840973882,0.774923810071778,0.658580776548479,0.629630152985919,0.499733952071983,0.510624980984721,0.411446596984752,0.809380005672574,0.561154086573515,0.479472318326589,0.472560543892905,0.690219347039238,0.38848238249775,0.454082201223355,0.491959653154481,0.626615916739684,0.883244762639515,0.772496249224059,0.499412458110601,0.63123823980568,0.737622628465761,0.52410622498719,0.684233540145215,0.819388168200385,0.732843503775075,0.697777427977417,0.855481607164256,0.742198365321383,0.46253231051378,0.784542894922197,0.650998510129284,0.763394735998008,0.576694813068025,0.453597295412328,0.571563445776701,0.414447674376424,0.570279003912583,0.280772946414072,0.593041068071034,0.467948628170416,0.637834442208987,0.345736649108585,0.861162547511049,0.440605670853984,0.512588860234246,0.792052329157014,0.638277566852048,0.511509455507621,0.635749558976386,0.738273805694189,0.516849947173614,0.519041978917085,0.351990975544322,0.747328023076989,0.385149879159872,0.859597484359983,0.570236752380151,0.268265313876327,0.629567004437558,0.680886049312539,0.717553662240971,0.635990699462127,0.817546895926353,0.27845191040542,0.636539944866672,0.658270810602698,0.616599852452055,0.754652669304051,0.660876218555495,0.300269186997321,0.28182521922281,0.884954581665807,0.381121389719192,0.650366534455679,0.483189897763077,0.627292685548309,0.340799751190934,0.879740431439131,0.51298636890715,0.519893842563033,0.586511565558612,0.30358850650955,0.431497312069405,0.520631736051291,0.339839148602914,0.538513396726921,0.837008321820758,0.866668494418263,0.295951423561201,0.894516123901121,0.524983213935047,0.547835522238165],[0.0685734323364337,0.0685856556809111,0.0685867295543274,0.068588987872435,0.068596231424698,0.0686042742172015,0.0686070057848135,0.0686076404353754,0.0686094958480222,0.0686120274413156,0.0686125923896136,0.0686130465475117,0.0686219931824509,0.0686245973388467,0.0686332714476807,0.0686361414359523,0.068640055165024,0.068643667516029,0.068645919226097,0.0686559830140031,0.0686620457478596,0.0686653280466891,0.0686690864829251,0.0686711239854339,0.0686764785006036,0.0686771207875247,0.0686797073786431,0.068686567972197,0.0686878488186319,0.0686926280568253,0.0686938198386459,0.068707460822032,0.0687116586314873,0.068716911300788,0.068721339177247,0.068730527264395,0.0687346320434757,0.0687347555487875,0.0687348583681179,0.0687394635945315,0.0687415676083176,0.068747194784962,0.0687476297248957,0.0687507002963322,0.0687712593835113,0.068775296969139,0.0687808020826828,0.0687831796178247,0.0687901467078851,0.0687903346086269,0.0687957677704149,0.0687988215103667,0.0688073458397071,0.068809406495681,0.0688097655705679,0.0688099928744013,0.0688130942604858,0.0688338012542912,0.0688363466530517,0.068842342385878,0.0688493888077014,0.0688657018604635,0.0688677825829007,0.068871267314481,0.0688740580027899,0.0688797916829967,0.0688807705164102,0.0688904421979584,0.0688915173704108,0.0688982666888241,0.0689052977400322,0.0689122777112318,0.0689144960999565,0.0689322837147918,0.0689360509686703,0.0689390867240663,0.0689426295154904,0.0689525405181225,0.0689791696002073,0.0689831341987249,0.0689863536017389,0.0689988251574844,0.0690105970029856,0.0690116224748589,0.0690138350432777,0.0690171569615373,0.0690369331055461,0.0690412450543715,0.0690667097057202,0.0690727462356402,0.0691081738348346,0.0691496340673707,0.0691550293014299,0.0692407131902957,0.0692462130817916,0.0694130627968812,0.0694188644962126,0.0694193663415414,0.0694484742351725,0.0694689775151709]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>max_depth<\/th>\n      <th>eta<\/th>\n      <th>gamma<\/th>\n      <th>min_child_weight<\/th>\n      <th>subsample<\/th>\n      <th>colsample_bytree<\/th>\n      <th>mae<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","scrollX":true,"scrollCollapse":true,"columnDefs":[{"className":"dt-right","targets":[1,2,3,4,5,6,7]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="exploring-parameter-space" class="section level3">
<h3><span class="header-section-number">6.4.5</span> Exploring Parameter Space</h3>
<p>Let’s explore the parameter space some, shall we?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">search_results <span class="op">%&gt;%</span>
<span class="st">   </span><span class="kw">group_by_at</span>(
     <span class="kw">c</span>(<span class="st">&quot;surrogate_run&quot;</span>, 
       <span class="st">&quot;surrogate_iteration&quot;</span>,
       <span class="st">&quot;param_id&quot;</span>,
      <span class="kw">getParamIds</span>(xgboost_random_params)
       )
     ) <span class="op">%&gt;%</span>
<span class="st">   </span><span class="kw">summarise</span>(<span class="dt">mae =</span> <span class="kw">mean</span>(mae)) <span class="op">%&gt;%</span>
<span class="st">   </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">surrogate_run =</span> <span class="kw">factor</span>(surrogate_run)) <span class="op">%&gt;%</span>
<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> subsample, <span class="dt">y =</span> gamma, <span class="dt">size =</span> mae)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">col =</span> surrogate_run)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">y =</span> <span class="st">&quot;gamma&quot;</span>,
    <span class="dt">x =</span> <span class="st">&quot;subsample&quot;</span>,
    <span class="dt">col =</span> <span class="st">&quot;Surrogate Run&quot;</span>,
    <span class="dt">size =</span> <span class="st">&quot;Average MAE&quot;</span>
  )</code></pre></div>
<div class="figure"><span id="fig:fig-param-compare"></span>
<img src="zillow-prize_files/figure-html/fig-param-compare-1.png" alt="Gamma and subsample parameters converging with each surrogate run" width="672" />
<p class="caption">
Figure 6.1: Gamma and subsample parameters converging with each surrogate run
</p>
</div>
<p>How does performance increase with each iteration?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">search_results <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by_at</span>(
    <span class="kw">c</span>(<span class="st">&quot;surrogate_run&quot;</span>, 
      <span class="st">&quot;surrogate_iteration&quot;</span>,
      <span class="st">&quot;param_id&quot;</span>,
       <span class="kw">getParamIds</span>(xgboost_random_params)
      )
    ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">mae =</span> <span class="kw">mean</span>(mae)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">surrogate_run =</span> <span class="kw">factor</span>(surrogate_run)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(
    surrogate_run,
    surrogate_iteration
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">iteration =</span> <span class="kw">row_number</span>()
  ) <span class="op">%&gt;%</span>
<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> iteration, <span class="dt">y =</span> mae)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">alpha =</span> <span class="fl">0.2</span>, <span class="dt">size =</span> <span class="fl">0.8</span>, <span class="dt">colour =</span> <span class="st">&quot;grey&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">col =</span> surrogate_run)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">y =</span> <span class="st">&quot;MAE&quot;</span>,
    <span class="dt">x =</span> <span class="st">&quot;iteration&quot;</span>,
    <span class="dt">col =</span> <span class="st">&quot;Surrogate Run&quot;</span>
  )</code></pre></div>
<div class="figure"><span id="fig:fig-mae-by-run"></span>
<img src="zillow-prize_files/figure-html/fig-mae-by-run-1.png" alt="Mean Absolute Error Progressively Decreasing with Each Surrogate Run" width="672" />
<p class="caption">
Figure 6.2: Mean Absolute Error Progressively Decreasing with Each Surrogate Run
</p>
</div>
</div>
</div>
<div id="tuned-model" class="section level2">
<h2><span class="header-section-number">6.5</span> Tuned Model</h2>
<p>Get the parameter set with the lowest average <code>mae</code> across all folds</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tuned_params &lt;-<span class="st"> </span>search_summary <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(mae <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(mae)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="kw">getParamIds</span>(xgboost_random_params)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as.list</span>()

tuned_params</code></pre></div>
<pre><code>## $max_depth
## [1] 5
## 
## $eta
## [1] 0.07531416
## 
## $gamma
## [1] 2.009971
## 
## $min_child_weight
## [1] 100
## 
## $subsample
## [1] 0.8821157
## 
## $colsample_bytree
## [1] 0.5284899</code></pre>
<p>Since we didn’t save the actual models produced in our search, train a model with the tuned parameters</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d_prepped &lt;-<span class="st"> </span><span class="kw">prep</span>(rec)

train_df &lt;-<span class="st"> </span><span class="kw">bake</span>(d_prepped, <span class="dt">newdata =</span> d)

x_train &lt;-<span class="st"> </span>train_df <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(features_to_use) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as.matrix</span>()

y_train &lt;-<span class="st"> </span>train_df<span class="op">$</span>log_error

xgb_train_data &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(x_train, <span class="dt">label =</span> y_train)

tuned_model &lt;-<span class="st"> </span><span class="kw">xgb.train</span>(<span class="dt">params =</span> tuned_params,
                         <span class="dt">data =</span> xgb_train_data,
                         <span class="dt">objective =</span> <span class="st">&#39;reg:linear&#39;</span>,
                         <span class="dt">verbose =</span> <span class="ot">FALSE</span>,
                         <span class="dt">nthread =</span> <span class="dv">20</span>,
                         <span class="dt">nrounds =</span> <span class="dv">1000</span>)</code></pre></div>
<div id="making-predictions-with-tuned-model" class="section level3">
<h3><span class="header-section-number">6.5.1</span> Making Predictions with Tuned Model</h3>
<p>Since we already created our helper function <code>predict_dates()</code> we can apply it here with our new model</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">predict_list &lt;-<span class="st"> </span><span class="kw">lapply</span>(id_parcel_splits, <span class="cf">function</span>(i) {
  
  pred_df &lt;-<span class="st"> </span><span class="kw">predict_date</span>(
    <span class="dt">parcel_id =</span> i, 
    <span class="dt">predict_date =</span> predict_dates, 
    <span class="dt">mdl =</span> tuned_model,
    <span class="dt">features_to_use =</span> features_to_use
    )
  })


predict_df &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(predict_list) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate_at</span>(<span class="kw">vars</span>(<span class="st">`</span><span class="dt">201610</span><span class="st">`</span><span class="op">:</span><span class="st">`</span><span class="dt">201712</span><span class="st">`</span>), round, <span class="dt">digits =</span> <span class="dv">4</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">ParcelId =</span> <span class="kw">as.integer</span>(ParcelId)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as.data.frame</span>()

<span class="kw">write_csv</span>(predict_df, <span class="st">&quot;submissions/submit01.csv&quot;</span>)</code></pre></div>
</div>
</div>
<div id="model-comparison" class="section level2">
<h2><span class="header-section-number">6.6</span> Model Comparison</h2>
<p>This model produced a MAE of <code>0.0651839</code> on the public leaderboard.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tuned_mae &lt;-<span class="st"> </span><span class="fl">0.0651839</span>

<span class="co"># how much did we improve?</span>
(tuned_mae <span class="op">-</span><span class="st"> </span>base_mae) <span class="op">/</span><span class="st"> </span>base_mae</code></pre></div>
<pre><code>## [1] -0.1745969</code></pre>
<p>Hey not bad! Our hyperparameter optimization process resulted in a 17.5% reduction in Mean Abosolute Error when compared to our base line model. While our absolute position on the leaderboard still isn’t very high, it’s a solid first submission that we can build upon and test new models against. With the top 10 finsishers on the public leaderboard all averaging 184 submissions each (highest being 449!) this empahsizes the iterative nature of predictive modeling. We aren’t in 1^st place with our first submission but we didn’t expect to be.</p>
<p>At this point if we were competing the actual competition we would go back to the drawing board in start investigating what we could do differently to improve our score. In our specific case, things like not making the predicion date the first Wednesday of each month and looking at the average prediction across all days in the month would be the first thing I would check to se what kind of improvement resulted.</p>
<p>Other things like adding more interaction features or stacking our xgboost predictions as part of an ensemble of other models is an area to explore as well.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>XGBoost: A Scalable Tree Boosting System <a href="https://arxiv.org/abs/1603.02754" class="uri">https://arxiv.org/abs/1603.02754</a><a href="modeling.html#fnref1">↩</a></p></li>
<li id="fn2"><p>Greedy Function Approximation: A Gradient Boosting Machine <a href="http://statweb.stanford.edu/~jhf/ftp/trebst.pdf" class="uri">http://statweb.stanford.edu/~jhf/ftp/trebst.pdf</a><a href="modeling.html#fnref2">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="feature-selection.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["zillow-prize.pdf", "zillow-prize.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
