[
["index.html", "Zillow Prize Modeling Welcome", " Zillow Prize Modeling Jesse Piburn 2018-07-20 Welcome This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],
["intro.html", "Chapter 1 Introduction 1.1 Problem 1.2 Evaluation 1.3 Initital Thoughts 1.4 Note on Using External Features", " Chapter 1 Introduction This project is meant to serve as an example workflow for making predictive models. The sections of the book are roughly broken into the major steps that are a part of the modeling process. As with many things in life, their is rarely a one-size-fits-all, always correct, way of doing something and making a predictive model is no different. In light of that, I want to stress that the workflow and methods used in this book are meant to be illustrative not authoritative. The only always correct answer in predictive modeling is, “It depends.” 1.1 Problem The problem we will workthrough in this book is the Zillow Prize compeitition on that took place on Kaggle. Although it is now closed, it provides a good problemset for us to work through. From the site &gt;&gt;&gt;In this million-dollar competition, participants will develop an algorithm that makes predictions about the future sale prices of homes. The contest is structured into two rounds, the qualifying round which opens May 24, 2017 and the private round for the 100 top qualifying teams that opens on Feb 1st, 2018. In the qualifying round, you’ll be building a model to improve the Zestimate residual error. In the final round, you’ll build a home valuation algorithm from the ground up, using external data sources to help engineer new features that give your model an edge over the competition. 1.2 Evaluation The submission were evaluated based on the Mean Absolute Error between the predicted logerror and the actual logerror The logerror is defined as \\[logerror = log(Zestimate) - log(SalePrice)\\] while Mean Absolute Error is defined as \\[MAE = \\frac{{\\sum_{i=1}^{n} |y_i - x_i|}}{n} = \\frac{{\\sum_{i=1}^{n} |e_i|}}{n} \\] 1.3 Initital Thoughts “Location, Location, Location.” — Some Real Estate Guy The above quote is of course the number one rule of real estate. Though out this modeling process, let’s try to keep this idea in mind when we are exploring, creating new features, and modeling the data. An additional interesting thing about this problem is that it can be thought about as creating a model to predict where Zillow’s model is bad. We aren’t trying to predict home prices, we are predicting where Zillow had a bad estimate of home prices, so the residuals of their Zestimate model.Based on this idea, let’s create some new features 1.4 Note on Using External Features In the original competition, you were only allowed to use the features transformations of those included in the data they provided. Since our goal is to provide an illustrative workflow for making a predictive model in general and not actually competing in the competition, we are not going to adhere to that rule and use a few external sources of information. "],
["preprocessing.html", "Chapter 2 PreProcessing 2.1 The Raw Data 2.2 Renaming Variables 2.3 Extracting Geographic Information", " Chapter 2 PreProcessing The first step for any data driven project is getting to know the data. In this section we will look at the raw data that was provided by the competition and then do a little pre processing that will make the data easier to work with for the rest of the project 2.1 The Raw Data The raw data from zillow contains the following data (desrciptions from the the data page) properties_2016.csv - all the properties with their home features for 2016. Note: Some 2017 new properties don’t have any data yet except for their parcelid’s. Those data points should be populated when properties_2017.csv is available. properties_2017.csv - all the properties with their home features for 2017 (released on 10/2/2017) train_2016.csv - the training set with transactions from 1/1/2016 to 12/31/2016 train_2017.csv - the training set with transactions from 1/1/2017 to 9/15/2017 (released on 10/2/2017) sample_submission.csv - a sample submission file in the correct format zillow_data_dictionary.xlsx - Field Definitions and coded value meanings 2.1.1 Saving Raw Data Using feather The R binding for the feather data store provides the ability for very fast read and write of data. For speed purposes we will save all raw data into a .feather file format to make all other read faster library(feather) library(readr) dir.create(&quot;data-raw/feather&quot;) prop_16 &lt;- read_csv(&quot;data-raw/properties_2016.csv&quot;) prop_17 &lt;- read_csv(&quot;data-raw/properties_2017.csv&quot;) train_16 &lt;- read_csv(&quot;data-raw/train_2016_v2.csv&quot;) train_17 &lt;- read_csv(&quot;data-raw/train_2017.csv&quot;) write_feather(prop_16, &quot;data-raw/feather/properties_2016.feather&quot;) write_feather(prop_17, &quot;data-raw/feather/properties_2017.feather&quot;) write_feather(train_16, &quot;data-raw/feather/train_2016_v2.feather&quot;) write_feather(train_17, &quot;data-raw/feather/train_2017.feather&quot;) 2.2 Renaming Variables Many of the feature names are not very consistant. To take advatange of helpful functions from the tidyverse set of packages, such as starts_with() and one_of() Let’s rename them to something more consistant and easier to work with. 2.2.1 Renaming properties Features library(tidyverse) prop_16 &lt;- read_feather(&quot;data-raw/feather/properties_2016.feather&quot;) prop_17 &lt;- read_feather(&quot;data-raw/feather/properties_2017.feather&quot;) prop_16 &lt;- prop_16 %&gt;% rename( id_parcel = parcelid, build_year = yearbuilt, area_basement = basementsqft, area_patio = yardbuildingsqft17, area_shed = yardbuildingsqft26, area_pool = poolsizesum, area_lot = lotsizesquarefeet, area_garage = garagetotalsqft, area_firstfloor_finished_1 = finishedfloor1squarefeet, area_firstfloor_finished_2 = finishedsquarefeet50, area_living_finished_calc = calculatedfinishedsquarefeet, area_base = finishedsquarefeet6, area_living_finished = finishedsquarefeet12, area_living_perimeter = finishedsquarefeet13, area_total = finishedsquarefeet15, num_unit = unitcnt, num_story = numberofstories, num_room = roomcnt, num_bathroom = bathroomcnt, num_bedroom = bedroomcnt, num_bathroom_calc = calculatedbathnbr, num_bath = fullbathcnt, num_75_bath = threequarterbathnbr, num_fireplace = fireplacecnt, num_pool = poolcnt, num_garage = garagecarcnt, region_county = regionidcounty, region_city = regionidcity, region_zip = regionidzip, region_neighbor = regionidneighborhood, tax_total = taxvaluedollarcnt, tax_building = structuretaxvaluedollarcnt, tax_land = landtaxvaluedollarcnt, tax_property = taxamount, tax_year = assessmentyear, tax_delinquency = taxdelinquencyflag, tax_delinquency_year = taxdelinquencyyear, zoning_property = propertyzoningdesc, zoning_landuse = propertylandusetypeid, zoning_landuse_county = propertycountylandusecode, str_flag_fireplace = fireplaceflag, str_flag_tub = hashottuborspa, str_quality = buildingqualitytypeid, str_framing = buildingclasstypeid, str_material = typeconstructiontypeid, str_deck = decktypeid, str_story = storytypeid, str_heating = heatingorsystemtypeid, str_aircon = airconditioningtypeid, str_arch_style = architecturalstyletypeid ) # use 2016 names to rename 17 names(prop_17) &lt;- names(prop_16) 2.2.2 renaming train features trans_16 &lt;- read_feather(&quot;data-raw/feather/train_2016_v2.feather&quot;) trans_17 &lt;- read_feather(&quot;data-raw/feather/train_2017.feather&quot;) trans_16 &lt;- trans_16 %&gt;% rename( id_parcel = parcelid, date = transactiondate, log_error = logerror ) # use 2016 names to rename 17 names(trans_17) &lt;- names(trans_16) 2.2.3 Basic Transformations Based on the definitions in the zillow_data_dictionary.xlsx we can recode some of the features to have be more interpretable while we are exploring. 2.2.3.1 Properties library(forcats) prop_16 &lt;- prop_16 %&gt;% mutate( tax_delinquency = ifelse(tax_delinquency == &quot;Y&quot;, &quot;Yes&quot;, &quot;No&quot;) %&gt;% as_factor(), str_flag_fireplace = ifelse(str_flag_fireplace == &quot;Y&quot;, &quot;Yes&quot;, &quot;No&quot;) %&gt;% as_factor(), str_flag_tub = ifelse(str_flag_tub == &quot;Y&quot;, &quot;Yes&quot;, &quot;No&quot;) %&gt;% as_factor(), zoning_landuse = factor(zoning_landuse, levels = sort(unique(zoning_landuse))), zoning_landuse = fct_recode(zoning_landuse, &quot;Commercial/Office/Residential Mixed Used&quot; = &quot;31&quot;, &quot;Multi-Story Store&quot; = &quot;46&quot;, &quot;Store/Office (Mixed Use)&quot; = &quot;47&quot;, &quot;Duplex (2 Units Any Combination)&quot; = &quot;246&quot;, &quot;Triplex (3 Units Any Combination)&quot; = &quot;247&quot;, &quot;Quadruplex (4 Units Any Combination)&quot; = &quot;248&quot;, &quot;Residential General&quot; = &quot;260&quot;, &quot;Single Family Residential&quot; = &quot;261&quot;, &quot;Rural Residence&quot; = &quot;262&quot;, &quot;Mobile Home&quot; = &quot;263&quot;, &quot;Townhouse&quot; = &quot;264&quot;, &quot;Cluster Home&quot; = &quot;265&quot;, &quot;Condominium&quot; = &quot;266&quot;, &quot;Cooperative&quot; = &quot;267&quot;, &quot;Row House&quot; = &quot;268&quot;, &quot;Planned Unit Development&quot; = &quot;269&quot;, &quot;Residential Common Area&quot; = &quot;270&quot;, &quot;Timeshare&quot; = &quot;271&quot;, &quot;Bungalow&quot; = &quot;273&quot;, &quot;Zero Lot Line&quot; = &quot;274&quot;, &quot;Manufactured Modular Prefabricated Homes&quot; = &quot;275&quot;, &quot;Patio Home&quot; = &quot;276&quot;, &quot;Inferred Single Family Residential&quot; = &quot;279&quot;, &quot;Vacant Land - General&quot; = &quot;290&quot;, &quot;Residential Vacant Land&quot; = &quot;291&quot; ) ) prop_17 &lt;- prop_17 %&gt;% mutate( tax_delinquency = ifelse(tax_delinquency == &quot;Y&quot;, &quot;Yes&quot;, &quot;No&quot;) %&gt;% as_factor(), str_flag_fireplace = ifelse(str_flag_fireplace == &quot;Y&quot;, &quot;Yes&quot;, &quot;No&quot;) %&gt;% as_factor(), str_flag_tub = ifelse(str_flag_tub == &quot;Y&quot;, &quot;Yes&quot;, &quot;No&quot;) %&gt;% as_factor(), zoning_landuse = factor(zoning_landuse, levels = sort(unique(zoning_landuse))), zoning_landuse = fct_recode(zoning_landuse, &quot;Commercial/Office/Residential Mixed Used&quot; = &quot;31&quot;, &quot;Multi-Story Store&quot; = &quot;46&quot;, &quot;Store/Office (Mixed Use)&quot; = &quot;47&quot;, &quot;Duplex (2 Units Any Combination)&quot; = &quot;246&quot;, &quot;Triplex (3 Units Any Combination)&quot; = &quot;247&quot;, &quot;Quadruplex (4 Units Any Combination)&quot; = &quot;248&quot;, &quot;Residential General&quot; = &quot;260&quot;, &quot;Single Family Residential&quot; = &quot;261&quot;, &quot;Rural Residence&quot; = &quot;262&quot;, &quot;Mobile Home&quot; = &quot;263&quot;, &quot;Townhouse&quot; = &quot;264&quot;, &quot;Cluster Home&quot; = &quot;265&quot;, &quot;Condominium&quot; = &quot;266&quot;, &quot;Cooperative&quot; = &quot;267&quot;, &quot;Row House&quot; = &quot;268&quot;, &quot;Planned Unit Development&quot; = &quot;269&quot;, &quot;Residential Common Area&quot; = &quot;270&quot;, &quot;Timeshare&quot; = &quot;271&quot;, &quot;Bungalow&quot; = &quot;273&quot;, &quot;Zero Lot Line&quot; = &quot;274&quot;, &quot;Manufactured Modular Prefabricated Homes&quot; = &quot;275&quot;, &quot;Patio Home&quot; = &quot;276&quot;, &quot;Inferred Single Family Residential&quot; = &quot;279&quot;, &quot;Vacant Land - General&quot; = &quot;290&quot;, &quot;Residential Vacant Land&quot; = &quot;291&quot; ) ) 2.2.3.2 Transactions The transactions tables are where our response variable log_error (name changed from orginal logerror) and the dates of the transactions are recorded. To make them easier to work with, let’s combine all the transactions into one table and create a few basic transformations of the date (name changed from original transactiondate) library(lubridate) # combine transactions into one data frame trans &lt;- trans_16 %&gt;% bind_rows(trans_17) %&gt;% mutate( abs_log_error = abs(log_error), year = year(date), month_year = make_date(year(date), month(date)), month = month(date, label = TRUE), week = floor_date(date, unit = &quot;week&quot;), week_of_year = week(date), week_since_start = (min(date) %--% date %/% dweeks()) + 1, wday = wday(date, label = TRUE), day_of_month = day(date) ) Save our output write_feather(prop_16, &quot;data/properties_16.feather&quot;) write_feather(prop_17, &quot;data/properties_17.feather&quot;) write_feather(trans, &quot;data/transactions.feather&quot;) 2.3 Extracting Geographic Information As noted in the 1 we are going to break from the rules of the competition and use external information to (hopefully) help improve our predictions. Since the data we are using relate to locations of individual properties and we have each of their geographic coordinates, latitude and longitude let’s use those to get the U.S. Census Geographies they are apart of that we can make use of when adding external information. The original data contain the fields rawcensustractandblock and censustractandblock but after trying to parse those into a usable format and failing, I figured it was just easier to use the latitude and longitude fields and then join that to the Census information. library(sf) library(tidycensus) # NAD83 / California zone 5 (ftUS) # https://epsg.io/2229 crs_id &lt;- 2229 api_key &lt;- Sys.getenv(&quot;CENSUS_API_KEY&quot;) census_api_key(api_key) # some obs have no data at all included lat/long # the original lat / lon are mulitpled by 10e5 so divide to # get lat lon back when converting to sf properties &lt;- read_feather(&quot;data-raw/properties_2017&quot;) %&gt;% filter(!is.na(latitude)) %&gt;% mutate( lat = latitude / 10e5, lon = longitude / 10e5 ) %&gt;% st_as_sf( coords = c(&quot;lon&quot;, &quot;lat&quot;), crs = 4326, # WGS 84 remove = FALSE # keep lat/long fields ) %&gt;% st_transform(crs_id) census_bgs &lt;- get_acs( geography = &quot;block group&quot;, variables = &quot;B19013_001&quot;, state = &quot;CA&quot;, county = c(&quot;Los Angeles&quot;, &quot;Orange&quot;, &quot;Ventura&quot;), geometry = TRUE, keep_geo_vars = TRUE ) %&gt;% st_transform(crs_id) # inner join # due to lat / lon error some points didn&#39;t intersect # with block groups left = FALSE is inner join properties_geo &lt;- properties %&gt;% st_join(census_bgs, left = FALSE) # find all of the points that didn&#39;t intersect # buffer them and then join to closest block # then add back the already joined points # the buffer distance I just played around with until # all points joined with a block group properties_geo &lt;- properties %&gt;% filter(!parcelid %in% properties_geo$parcelid) %&gt;% st_buffer(dist = 1500) %&gt;% # units are in us-ft based on crs_id st_join(census_bgs, left = FALSE, largest = TRUE) %&gt;% rbind(properties_geo) # remove geometry b/c feather can&#39;t store lists properties_geo &lt;- properties_geo %&gt;% select( id_parcel = parcelid, id_geo_state = STATEFP, id_geo_county = COUNTYFP, id_geo_tract = TRACTCE, id_geo_bg = BLKGRPCE, id_geo_bg_fips = GEOID, id_geo_bg_name = NAME.y, geo_bg_arealand = ALAND, geo_bg_areawater = AWATER, lat, lon ) %&gt;% mutate( id_geo_county_fips = paste0(id_geo_state, id_geo_county), id_geo_tract_fips = paste0(id_geo_county_fips, id_geo_tract), id_geo_county_name = factor(id_geo_county) %&gt;% fct_recode( &quot;Los Angeles&quot; = &quot;037&quot;, &quot;Orange&quot; = &quot;059&quot;, &quot;Ventura&quot; = &quot;111&quot; ) ) Now save our geographic features # remove geometry b/c feather can&#39;t store lists # add back in when needed from lat lon properties_geo$geometry &lt;- NULL write_feather(properties_geo, &quot;data/properties_geo_only.feather&quot;) "],
["eda.html", "Chapter 3 Exploratory Analysis 3.1 Response Variable 3.2 Predictor Variables 3.3 Exploring log_error A little More", " Chapter 3 Exploratory Analysis After ?? the next step is doing exploratory data analysis (EDA). I can’t stess enough how critical this step is. It is tempting to want to jump right into making models and then start improving and tweaking them from there, but this can quickly take you down a rabbit hole, waste your time, and generally make you sad. The time you spend doing EDA will pay dividends later. Below we are first going to look at only our response variable log_error after that we will look at the predictor features in properties. Once we get a good handle on both of those, we’ll look at how our response variable log_error varies across the predictors in properties Throughout this section we will progressively pare down features in our properties data due to common things such as, missingness and redundancy, to only the features that we are going to continue with into the next stages. library(skimr) library(tidyverse) library(feather) library(DataExplorer) trans &lt;- read_feather(&quot;data/transactions.feather&quot;) 3.1 Response Variable log_error something something text come back to when the processing is finished skim(trans) %&gt;% skimr::pander() Skim summary statistics n obs: 167888 n variables: 12 Table continues below variable missing complete n min max date 0 167888 167888 2016-01-01 2017-09-25 month_year 0 167888 167888 2016-01-01 2017-09-01 week 0 167888 167888 2015-12-27 2017-09-24 median n_unique 2016-10-11 616 2016-10-01 21 2016-10-09 92 Table continues below variable missing complete n n_unique month 0 167888 167888 12 wday 0 167888 167888 7 top_counts ordered Jun: 22378, May: 20448, Aug: 20412, Jul: 19437 FALSE Fri: 44914, Thu: 34143, Wed: 32692, Tue: 31404 FALSE Table continues below variable missing complete n mean sd p0 day_of_month 0 167888 167888 16.43 8.99 1 id_parcel 0 167888 167888 1.3e+07 3e+06 1.1e+07 p25 p50 p75 p100 9 16 24 31 1.2e+07 1.3e+07 1.4e+07 1.7e+08 Table continues below variable missing complete n mean sd p0 abs_log_error 0 167888 167888 0.069 0.15 0 log_error 0 167888 167888 0.014 0.17 -4.66 week_of_year 0 167888 167888 22.15 11.5 1 week_since_start 0 167888 167888 46.31 26.84 1 year 0 167888 167888 2016.46 0.5 2016 p25 p50 p75 p100 0.014 0.032 0.069 5.26 -0.025 0.006 0.039 5.26 13 22 31 53 23 41 72 91 2016 2016 2017 2017 trans %&gt;% ggplot(aes(x = log_error)) + geom_histogram(bins=400, fill = &quot;red&quot;, alpha = 0.5) + theme_bw() Figure 3.1: Distribution of Log Error trans %&gt;% filter( log_error &gt; quantile(log_error, probs = c(.05)), log_error &lt; quantile(log_error, probs = c(.95)) ) %&gt;% ggplot(aes(x = log_error)) + geom_histogram(fill = &quot;red&quot;, alpha = 0.5) + theme_bw() Figure 3.2: Distribution of Log Error Between 5 and 95 Percentile trans %&gt;% filter( log_error &gt; quantile(abs_log_error, probs = c(.05)), log_error &lt; quantile(abs_log_error, probs = c(.95)) ) %&gt;% ggplot(aes(x = abs_log_error)) + geom_histogram(fill = &quot;red&quot;, alpha = 0.5) + theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 3.3: Distribution of Absolute value of Log Error Between 5 and 95 Percentile trans %&gt;% group_by(month_year) %&gt;% summarise(mean_log_error = mean(log_error)) %&gt;% ggplot(aes(x = month_year, y = mean_log_error)) + geom_line(size = 1, colour = &quot;red&quot;) + geom_point(size = 3, colour = &quot;red&quot;) + theme_bw() Figure 3.4: Average Log Error by Month trans %&gt;% group_by(month_year, year, month) %&gt;% summarise(mean_log_error = mean(log_error)) %&gt;% ungroup() %&gt;% ggplot(aes(x = as.numeric(month), y = mean_log_error)) + geom_path(aes(colour = as.factor(year)), size = 1) + theme_bw() + ylim(c(0, .03)) + scale_x_continuous(breaks = 1:12, labels = levels(trans$month)) + labs( colour = NULL, x = &quot;month&quot; ) Figure 3.5: Average Log Error by Month. 2017 Looks to have a higher baseline trans %&gt;% group_by(week_since_start) %&gt;% summarise(mean_log_error = mean(log_error)) %&gt;% ggplot(aes(x = week_since_start, y = mean_log_error)) + geom_line(colour = &quot;red&quot;, size = 1) + geom_smooth() + theme_bw() Figure 3.6: Average Log Error by Week 3.1.1 Transactions Over Time trans %&gt;% group_by(week_since_start) %&gt;% summarise(n = n()) %&gt;% ggplot(aes(x = week_since_start, y = n)) + geom_line(colour = &quot;red&quot;, size = 1) + theme_bw() + labs( y = &quot;Numeber of Transactions&quot; ) Figure 3.7: Number of Transactions per Week. The dip in the middle coorisponds to the hold out testing data trans %&gt;% group_by(wday) %&gt;% count() %&gt;% ggplot(aes(x = wday, y = n)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;red&quot;, alpha = 0.5) + theme_bw() Figure 3.8: Number of Transactions by Day of the Week. 3.1.2 Spatial Distribution of log_error library(leaflet) library(leaflet.extras) read_feather(&quot;data/properties_geo_only.feather&quot;) %&gt;% right_join(trans, by = &quot;id_parcel&quot;) %&gt;% filter( !is.na(lat) ) %&gt;% leaflet() %&gt;% addProviderTiles(providers$CartoDB.DarkMatter) %&gt;% addHeatmap(lng=~lon, lat=~lat, intensity = ~log_error, radius = 5, minOpacity = 0.2, cellSize = 6) Figure 3.9: Distribution of Log Errors 3.2 Predictor Variables Now lets take a look at the properties dataset. Based on the descriptions from Kaggle, it seem’s like the properties_17.csv has updated information and is a replacement from that of properties_16.csv. For our purposes we are only going to use properties_17.csv, however given more space, it would be interesting to look into the differences in these files to see if there were any patterns that could be useful. properties &lt;- read_feather(&quot;data/properties_17.feather&quot;) skim(properties) %&gt;% skimr::pander() ## Warning: Skimr&#39;s histograms incorrectly render with pander on Windows. ## Removing them. Use kable() if you&#39;d like them rendered. Skim summary statistics n obs: 2985217 n variables: 58 variable missing complete n min max empty n_unique fips 2932 2982285 2985217 5 5 0 3 pooltypeid10 2968211 17006 2985217 1 1 0 1 pooltypeid2 2952161 33056 2985217 1 1 0 1 rawcensustractandblock 2932 2982285 2985217 12 16 0 100529 str_arch_style 2979156 6061 2985217 1 2 0 8 str_material 2978471 6746 2985217 1 2 0 5 zoning_landuse_county 2999 2982218 2985217 1 4 0 234 zoning_property 1002746 1982471 2985217 1 10 0 5651 Table continues below variable missing complete n n_unique str_flag_fireplace 2980054 5163 2985217 1 str_flag_tub 2935155 50062 2985217 1 tax_delinquency 2928702 56515 2985217 1 zoning_landuse 2932 2982285 2985217 16 top_counts ordered NA: 2980054, No: 5163 FALSE NA: 2935155, No: 50062 FALSE NA: 2928702, Yes: 56515 FALSE Sin: 2152863, Con: 483789, Dup: 114415, Pla: 61559 FALSE Table continues below variable missing complete n mean area_base 2963735 21482 2985217 2427.56 area_basement 2983590 1627 2985217 647.22 area_firstfloor_finished_1 2781459 203758 2985217 1379.78 area_firstfloor_finished_2 2781459 203758 2985217 1392.03 area_garage 2094209 891008 2985217 383.16 area_living_finished 264431 2720786 2985217 1764.04 area_living_perimeter 2977546 7671 2985217 1178.92 area_patio 2903629 81588 2985217 321.54 area_pool 2957259 27958 2985217 519.72 area_shed 2982571 2646 2985217 278.37 area_total 2795032 190185 2985217 2754.87 id_parcel 0 2985217 2985217 1.3e+07 latitude 2932 2982285 2985217 3.4e+07 longitude 2932 2982285 2985217 -1.2e+08 num_75_bath 2668860 316357 2985217 1.01 num_bath 117156 2868061 2985217 2.25 num_fireplace 2672093 313124 2985217 1.17 num_garage 2094209 891008 2985217 1.83 num_pool 2445585 539632 2985217 1 num_story 2299541 685676 2985217 1.4 num_unit 1004175 1981042 2985217 1.18 pooltypeid7 2479322 505895 2985217 1 region_city 62128 2923089 2985217 34987.66 region_county 2932 2982285 2985217 2569.09 region_neighbor 1828476 1156741 2985217 193538.7 region_zip 12714 2972503 2985217 96553.29 str_aircon 2169855 815362 2985217 1.95 str_deck 2967838 17379 2985217 66 str_framing 2972486 12731 2985217 3.73 str_heating 1116053 1869164 2985217 4.08 str_quality 1043822 1941395 2985217 6.28 str_story 2983594 1623 2985217 7 tax_delinquency_year 2928700 56517 2985217 13.89 tax_year 2933 2982284 2985217 2016 sd p0 p25 p50 p75 p100 7786.19 117 1072 2008 3411 952576 538.79 20 272 535 847.5 8516 634.42 1 1010 1281 1615 31303 682.32 3 1012 1284 1619 41906 246.22 0 312 441 494 7749 1031.38 1 1198 1542 2075 427079 357.09 120 960 1296 1440 2688 236.88 10 190 270 390 7983 191.33 19 430 495 594 17410 369.78 10 96 168 320 6141 5999.38 112 1696 2173 2975 820242 7909966.39 1.1e+07 1.2e+07 1.3e+07 1.4e+07 1.7e+08 243515.71 3.3e+07 3.4e+07 3.4e+07 3.4e+07 3.5e+07 345591.77 -1.2e+08 -1.2e+08 -1.2e+08 -1.2e+08 -1.2e+08 0.12 1 1 1 1 7 0.99 1 2 2 3 32 0.46 1 1 1 1 9 0.61 0 2 2 2 25 0 1 1 1 1 1 0.54 1 1 1 2 41 2.49 1 1 1 1 997 0 1 1 1 1 1 50709.68 3491 12447 25218 45457 4e+05 788.68 1286 1286 3101 3101 3101 165725.27 6952 46736 118920 274800 764167 3680.82 95982 96180 96377 96974 4e+05 3.16 1 1 1 1 13 0 66 66 66 66 66 0.5 1 3 4 4 5 3.29 1 2 2 7 24 1.73 1 5 6 8 12 0 7 7 7 7 7 2.56 0 14 14 15 99 0.06 2000 2016 2016 2016 2016 Table continues below variable missing complete n mean area_living_finished_calc 45097 2940120 2985217 1831.46 area_lot 272706 2712511 2985217 22603.76 build_year 47833 2937384 2985217 1964.44 censustractandblock 74985 2910232 2985217 6e+13 num_bathroom 2957 2982260 2985217 2.22 num_bathroom_calc 117156 2868061 2985217 2.3 num_bedroom 2945 2982272 2985217 3.09 num_room 2969 2982248 2985217 1.47 tax_building 46464 2938753 2985217 178142.89 tax_land 59926 2925291 2985217 268455.77 tax_property 22752 2962465 2985217 5408.95 tax_total 34266 2950951 2985217 443527.93 sd p0 p25 p50 p75 p100 1954.2 1 1215 1574 2140 952576 249983.63 100 5683 7000 9893 3.7e+08 23.64 1801 1950 1963 1981 2016 3.2e+11 -1 6e+13 6e+13 6.1e+13 4.8e+14 1.08 0 2 2 3 32 1 1 2 2 3 32 1.27 0 2 3 4 25 2.84 0 0 0 0 96 460050.31 1 77666 127066 2e+05 2.6e+08 486509.71 1 79700 176619 326100 9.4e+07 9675.57 0.24 2468.62 4007.62 6230.5 3823175.65 816336.63 1 188220 321161 514072 3.2e+08 3.2.1 Missingness Missing values in data is a cold cruel reality. It is one of the most contraining factors there is when it comes to predictive power. Having a good understanding of the prevalence of missing values and any patterns to them is needed to make the most out of what data you do have. missing_data &lt;- plot_missing(properties, theme = theme_bw()) Figure 3.10: Completeness by Feature. Many are extremely sparse missing_data ## # A tibble: 58 x 4 ## feature num_missing pct_missing group ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 id_parcel 0 0 Good ## 2 str_aircon 2169855 0.727 Bad ## 3 str_arch_style 2979156 0.998 Remove ## 4 area_basement 2983590 0.999 Remove ## 5 num_bathroom 2957 0.000991 Good ## 6 num_bedroom 2945 0.000987 Good ## 7 str_framing 2972486 0.996 Remove ## 8 str_quality 1043822 0.350 OK ## 9 num_bathroom_calc 117156 0.0392 Good ## 10 str_deck 2967838 0.994 Remove ## # ... with 48 more rows There seem to be quite a lot of missing features. For now lets remove the ones that are over 50% and continue on with those. We could come back to the ones we dropped and try to recover some of those missing values with more sophisticated methods, for example we could impute the missing values based on their spatial neighbors but for now we will continue with the ones that have over 50% of their values. A few of the features, rawcensustractandblock, fips, and censustractandblock, and region_county are ID fields for their census geography units. Since we have already extracted that information earlier in properties_geo we will drop them here as well since we can add the information contained in those features in a cleaner format later. Additionally, based on the descriptions, zoning_landuse, zoning_landuse_county, and zoning_property all seem to contain pretty similar information.Since the number of unique categories are fairly large for each one, if they are redundant they could add needless complexity and computation time to our model. Let’s use a chi-squared test to see what it looks like chisq.test(properties$zoning_landuse, properties$zoning_property) ## ## Pearson&#39;s Chi-squared test ## ## data: properties$zoning_landuse and properties$zoning_property ## X-squared = 4377000, df = 73450, p-value &lt; 2.2e-16 chisq.test(properties$zoning_landuse, properties$zoning_landuse_county) ## ## Pearson&#39;s Chi-squared test ## ## data: properties$zoning_landuse and properties$zoning_landuse_county ## X-squared = 37455000, df = 3262, p-value &lt; 2.2e-16 Based on that, let’s remove zoning_property and zoning_landuse_county features_to_keep &lt;- missing_data %&gt;% filter( pct_missing &lt;= .50, !feature %in% c(&quot;rawcensustractandblock&quot;, &quot;fips&quot;, &quot;censustractandblock&quot;, &quot;region_county&quot;, &quot;zoning_property&quot;, &quot;zoning_landuse_county&quot;) ) %&gt;% select(feature) %&gt;% .$feature %&gt;% as.character() ## Warning: package &#39;bindrcpp&#39; was built under R version 3.4.4 properties &lt;- properties %&gt;% select(features_to_keep) 3.2.2 Numeric Features Lets look at the histograms of all the numeric features properties %&gt;% select( -id_parcel ) %&gt;% plot_histogram(ggtheme = theme_bw(), fill = &quot;red&quot;, alpha = 0.5) Figure 3.11: Distriubtions of All Numeric Features Figure 3.11: Distriubtions of All Numeric Features Looking at the histograms a few things become obvious. There are huge outliers in many of the features and there are some features that are currently encoded as numeric but should not be treated as such. For example, str_quality is an ordinal scale 1 (best qaulity) to 12 (worst) but if we leave them as numeric they will be treated as ratio. str_heating is nominal so the order doesn’t have meaning. Other that need to be changed are region_city, region_zip Once we do this, we’ll look again at the relationships between our numeric features. properties &lt;- properties %&gt;% mutate( str_quality = factor(str_quality, levels = min(str_quality, na.rm = TRUE):max(str_quality, na.rm = TRUE), ordered = TRUE), str_heating = factor(str_heating, levels = na.omit(unique(str_heating)), ordered = FALSE), region_city = factor(region_city, levels = na.omit(unique(region_city)), ordered = FALSE), region_zip = factor(region_zip, levels = na.omit(unique(region_zip)), ordered = FALSE) ) 3.2.3 Numeric Outliers Based on the histograms there looks to be lots of outliers in many of our numeric features. Two groups of features pop out, the num_* features and the tax_*features. Let’s take a closer look. properties %&gt;% select(starts_with(&quot;num_&quot;)) %&gt;% plot_histogram(ggtheme = theme_bw(), fill = &quot;red&quot;, alpha = 0.5) Figure 3.12: Distriubtions of ’num_*’ Features Looking at the num_bathroom, num_bathroom_calc, num_bath is pretty interesting. num_bathroom was one of the most complete features we had however, looking at the distributions, it seems strange that there would be so many houses with 0 bathrooms. sum(properties$num_bathroom == 0, na.rm = TRUE) ## [1] 113470 sum(properties$num_bathroom_calc == 0, na.rm = TRUE) ## [1] 0 Now for comparing all 3 properties %&gt;% group_by( num_bathroom_calc, num_bath, num_bathroom ) %&gt;% count() %&gt;% DT::datatable() If you sort by descending by n you’ll see that one of the most frequent combinations is blank values of num_bathroom_calc and num_bath which are NA values and 0 for num_bathroom. Based on this I am interpreting that as either 0 being a coded value for NA or it just being wrong. Either way it looks like num_bathroom_calc is the one to keep out of all 3, since it has calculations of half-baths as well. Applying the same logic to num_room and num_bedroom we can set all values equal to 0 to NA. One side effect of this is that the num_room feature is now almost 100% missing and not very useful anymore. So we will just remove it. Quickly looking at area_living_finished_calc and area_living_finished reveals a similar *_calc being a corrected version of the feature. Becasue of this we will go ahead and remove area_living_finished as well properties &lt;- properties %&gt;% select( -num_bath, -num_bathroom, -num_room, -area_living_finished ) %&gt;% mutate( num_bedroom = ifelse(num_bedroom == 0, NA, num_bedroom) ) Now let’s look at the tax related features properties %&gt;% select(starts_with(&quot;tax_&quot;)) %&gt;% plot_histogram(ggtheme = theme_bw(), fill = &quot;red&quot;, alpha = 0.5) Figure 3.13: Distriubtions of ’tax_*’ Features Lets look at the highest values for tax_total and see if something jumps out properties %&gt;% mutate(tax_rank = rank(desc(tax_total))) %&gt;% filter(tax_rank &lt;= 20) %&gt;% select( zoning_landuse, starts_with(&quot;area_&quot;), starts_with(&quot;tax_&quot;) ) %&gt;% arrange(tax_rank) %&gt;% DT::datatable( extensions = &#39;FixedColumns&#39;, options = list( dom = &#39;t&#39;, scrollX = TRUE, scrollCollapse = TRUE ) ) While the values are extremely large, they appear to look legitimate. We won’t remove these, but it does indicate that we should perhaps apply some transformations to our tax features before we start applying our model. Now a look at the relationships between our remaining numeric features library(heatmaply) properties %&gt;% select(-id_parcel) %&gt;% select_if(is.numeric) %&gt;% cor(use = &quot;pairwise.complete.obs&quot;) %&gt;% heatmaply_cor() Figure 3.14: Correlation of Numeric Features 3.2.4 Categorical Features plot_bar(properties, ggtheme = theme_bw()) ## 2 columns ignored with more than 50 categories. ## region_city: 187 categories ## region_zip: 404 categories Figure 3.15: Distriubtions of All Categorical Features The distribution across categories are extremely non-uniform, especially str_heating and zoning_landuse. This imbalance could cause use some pain later one when trying to fit our model. One way we can avoid some of this pain is by collapsing some of the rare categories into an other category. The number of categories we collapse to is not a hard and fast decision, it can be based on number of observations, subject matter expertise, heterogentity of the response variable within categories, or some mix of all of these). Let’s look at what the distribution of log_error looks like across these categories. library(ggridges) properties %&gt;% select( id_parcel, str_quality ) %&gt;% right_join(trans, by = &quot;id_parcel&quot;) %&gt;% ggplot(aes(x = log_error, y = fct_reorder(str_quality, log_error), fill = factor(..quantile..))) + stat_density_ridges( geom = &quot;density_ridges_gradient&quot;, calc_ecdf = TRUE, quantiles = c(0.05, 0.95) ) + scale_fill_manual( name = &quot;Probability&quot;, values = c(&quot;#FF0000A0&quot;, &quot;#A0A0A0A0&quot;, &quot;#0000FFA0&quot;), labels = c(&quot;(0, 0.05]&quot;, &quot;(0.05, 0.95]&quot;, &quot;(0.95, 1]&quot;) ) + xlim(c(-0.5, 0.5)) + theme_bw() + labs( y = &quot;str_quality&quot; ) Figure 3.16: Distribution of Log Error Across Structure Quality Feature library(ggridges) properties %&gt;% select( id_parcel, str_heating ) %&gt;% right_join(trans, by = &quot;id_parcel&quot;) %&gt;% ggplot(aes(x = log_error, y = fct_reorder(str_heating, log_error), fill = factor(..quantile..))) + stat_density_ridges( geom = &quot;density_ridges_gradient&quot;, calc_ecdf = TRUE, quantiles = c(0.05, 0.95) ) + scale_fill_manual( name = &quot;Probability&quot;, values = c(&quot;#FF0000A0&quot;, &quot;#A0A0A0A0&quot;, &quot;#0000FFA0&quot;), labels = c(&quot;(0, 0.05]&quot;, &quot;(0.05, 0.95]&quot;, &quot;(0.95, 1]&quot;) ) + xlim(c(-0.5, 0.5)) + theme_bw() + labs( y = &quot;str_heating&quot; ) Figure 3.17: Distribution of Log Error Across Heating Type Feature library(ggridges) properties %&gt;% select( id_parcel, zoning_landuse ) %&gt;% right_join(trans, by = &quot;id_parcel&quot;) %&gt;% ggplot(aes(x = log_error, y = fct_reorder(zoning_landuse, log_error), fill = factor(..quantile..))) + stat_density_ridges( geom = &quot;density_ridges_gradient&quot;, calc_ecdf = TRUE, quantiles = c(0.05, 0.95) ) + scale_fill_manual( name = &quot;Probability&quot;, values = c(&quot;#FF0000A0&quot;, &quot;#A0A0A0A0&quot;, &quot;#0000FFA0&quot;), labels = c(&quot;(0, 0.05]&quot;, &quot;(0.05, 0.95]&quot;, &quot;(0.95, 1]&quot;) ) + xlim(c(-0.5, 0.5)) + theme_bw() + labs( y = &quot;zoning_landuse&quot; ) Figure 3.18: Distribution of Log Error Across Zoning Feature Since the distributions of log_error within each category seems well behaved, we will recode them based on number of observations properties &lt;- properties %&gt;% mutate( str_heating = fct_lump(str_heating, n = 6), zoning_landuse = fct_lump(zoning_landuse, n = 8), str_heating = fct_recode(str_heating, &quot;Central&quot; = &quot;2&quot;, &quot;Floor/Wall&quot; = &quot;7&quot;, &quot;Solar&quot; = &quot;20&quot;, &quot;Forced Air&quot; = &quot;6&quot;, &quot;Yes - Type Unknown&quot; = &quot;24&quot;, &quot;None&quot; = &quot;13&quot; ) ) 3.3 Exploring log_error A little More Now let’s join the properties and properties_geo tables to our trans table of tranactions and their log_error’s and explore those trans_prop &lt;- read_feather(&quot;data/properties_geo_only.feather&quot;) %&gt;% right_join(trans, by = &quot;id_parcel&quot;) %&gt;% left_join(properties, by = &quot;id_parcel&quot;) trans_prop %&gt;% group_by(id_geo_bg_fips, id_geo_county_name) %&gt;% summarise( n = n(), mean_abs_error = mean(abs_log_error) ) %&gt;% ungroup() %&gt;% mutate( trans_pert = cut(n, breaks = c(seq(0, 100, 10), 350)) ) %&gt;% ggplot(aes(x = trans_pert, y = mean_abs_error, colour = id_geo_county_name)) + geom_boxplot(outlier.size = 1.5, outlier.alpha = 1/3) + theme_bw() + labs( subtitle = &quot;Block Group Average Mean Absolute Error&quot;, colour = NULL, x = &quot;Number of Total Transactions per Block Group&quot;, y = &quot;Mean Absolute Log Error&quot; ) Figure 3.19: Outliers and Variability of Mean Absolute Error Dreceases When Neighborhood Sales Increase It looks like Los Angeles is largerly the only county that has information populated for str_qaulity trans_prop %&gt;% ggplot(aes(x = str_quality, y = log_error, colour = id_geo_county_name)) + geom_boxplot(outlier.size = 1.5, outlier.alpha = 1/3) + theme_bw() + labs( colour = NULL ) Figure 3.20: Log Error by Structure Quality library(ggmap) trans_prop_tmp &lt;- trans_prop %&gt;% filter(!is.na(id_geo_county_name)) %&gt;% group_by( id_parcel, id_geo_county_name ) %&gt;% mutate( log_error_parcel_avg = mean(log_error) ) %&gt;% ungroup() %&gt;% mutate( outlier = ifelse(log_error &lt; quantile(log_error, probs = .1) | log_error &gt; quantile(log_error, probs = .9), &quot;Outlier&quot;, &quot;Normal&quot;) ) error_map &lt;- get_map(location = &quot;Los Angeles, CA&quot;, color=&quot;bw&quot;, crop = FALSE, zoom = 9) ggmap(error_map) + stat_density2d( data = trans_prop_tmp, aes(x = lon, y = lat, fill = ..level.., alpha = ..level..), geom = &quot;polygon&quot;, size = 0.001, bins = 100 ) + scale_fill_viridis_c() + scale_alpha(range = c(0.05, 0.95), guide = FALSE) + facet_wrap(~outlier) Figure 3.21: Spatial Distribution of Log Error Outliers Now lets look at the spatiotemporal distribution of log_error outliers trans_prop %&gt;% filter( !is.na(lat), ( log_error &lt;= quantile(log_error, probs = .1) | log_error &gt;= quantile(log_error, probs = .9) ) ) %&gt;% mutate( lon = round(lon/0.5, digits = 1) * 0.5, lat = round(lat/0.5, digits = 1) * 0.5 ) %&gt;% group_by(lon, lat, month_year) %&gt;% summarise( n = n() ) %&gt;% ggplot(aes(lon, lat)) + geom_raster(aes(fill = n)) + scale_fill_viridis_c() + facet_wrap(~month_year, ncol = 3) + coord_quickmap() + theme_dark() + labs( subtitle = &quot;Downtown Los Angeles looks to be consistently bad&quot;, fill = &quot;Count&quot; ) + theme( axis.text = element_text(size = 5) ) Figure 3.22: SpatioTemporal Distribution of Log Error Outliers At first glance there looks to be a strong spatial and temporal correlation to log_error. Let’s look more into the spatial correlation. Moran’s I and its variant Local Moran’s I, provide a useful measure of the amount of spatial autocorrelation in a variable. library(spatstat) library(spdep) d &lt;- trans_prop %&gt;% filter( !is.na(lat), ( log_error &lt;= quantile(log_error, probs = .1) | log_error &gt;= quantile(log_error, probs = .9) ) ) %&gt;% mutate( lon = round(lon/0.1, digits = 1) * 0.1, lat = round(lat/0.1, digits = 1) * 0.1 ) %&gt;% group_by(lon, lat) %&gt;% summarise( n = n() ) coordinates(d) &lt;- ~lon + lat w &lt;- knn2nb(knearneigh(d, k = 10, longlat = TRUE)) moran.test(d$n, nb2listw(w)) ## ## Moran I test under randomisation ## ## data: d$n ## weights: nb2listw(w) ## ## Moran I statistic standard deviate = 71.112, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 4.308120e-01 -1.952362e-04 3.673489e-05 local_moran &lt;- as.data.frame(localmoran(d$n, nb2listw(w))) d %&gt;% as.data.frame() %&gt;% cbind(local_moran) %&gt;% ggplot(aes(lon, lat)) + geom_raster(aes(fill = Ii)) + scale_fill_viridis_c() + coord_quickmap() + theme_dark() + labs( title = &quot;Local Moran&#39;s I on Outliers Density&quot;, fill = &quot;Local Moran&#39;s I&quot; ) Figure 3.23: Spatial Autocorrelation of Log Error Outliers Let’s save our pared down properties table and then get into feature engineering write_feather(properties, &quot;data/properties_17_filtered.feather&quot;) "],
["feat-eng.html", "Chapter 4 Feature Engineering 4.1 Creating New Features 4.2 Handling Missing Data 4.3 Feature Transformation", " Chapter 4 Feature Engineering After we have done an intital EDA of our data we can start doing some feature engineering, this is where we can create new features such as interaction variables, apply transformations such as centering and scaling, choice how we want to encode our categorical features, and also bring in new external information. Just as in 3, throughout this section we will progressively updating our properties data to include new and transformed features that we are going to continue with into the next stages. 4.1 Creating New Features “Everything is related to everything else, but near things are more related than distant things.” Waldo Tobler This “First Law of Geography” is something we can take advantage of for creating new features based on our existing ones. In this section we will create based on both data from the Kaggle competition and also examples of external sources as well 4.1.1 Internal Features Since we have the neighborhood, as defined by id_geo_bg_fips, that each parcel is apart of, we can use this to create neighborhood average features. There are many ways one could define neighborhood for the purposes of using near by parcels, knn for example. Perhaps a more rigourous and certainly more computationly intensive approach would be to estimate the radius at which the spatial autocorrelation of log_error is no longer statistically significant using something such as a bootstrapped spline correlogram such as the function spline.correlog() provided by the ncf package ncf::spline.correlog(x = lon, y = lat, z = log_error) For now we will stick with defining our neighborhood by the census block group (id_geo_bg_fips) that each parcel is apart of 4.1.1.1 Neigborhood Average properties Features bg_avg_features &lt;- properties %&gt;% group_by(id_geo_bg_fips) %&gt;% select(-id_parcel) %&gt;% select_if(is.numeric) %&gt;% summarise_all(mean, na.rm = TRUE) %&gt;% filter( !is.na(id_geo_bg_fips) ) names(bg_avg_features) &lt;- paste0(&quot;bg_avg_&quot;, names(bg_avg_features)) names(bg_avg_features)[1] &lt;- &quot;id_geo_bg_fips&quot; # update the properties table properties &lt;- properties %&gt;% left_join(bg_avg_features, by = &quot;id_geo_bg_fips&quot;) 4.1.1.2 Rolling Local Average log_error There is a strong spatial and temporal autocorrelation to our response variable log_error. To take advantage of this, let’s create a few new features based on the rolling average of the local log_error values. Because these features will have values for every day from min(trans$date) to max(trans$date) we won’t join them to our data yet. library(tibbletime) trans_prop &lt;- properties_geo %&gt;% right_join(trans, by = &quot;id_parcel&quot;) %&gt;% select( id_parcel, id_geo_bg_fips, id_geo_tract_fips, date, log_error ) # create rolling functions ------------------------------------------------ rolling_sum_7 &lt;- rollify(~sum(.x, na.rm = TRUE), window = 7) rolling_sum_28 &lt;- rollify(~sum(.x, na.rm = TRUE), window = 28) # by block group ---------------------------------------------------------- roll_bg &lt;- create_series(min(trans_prop$date) ~ max(trans_prop$date), &#39;daily&#39;, class = &quot;Date&quot;) %&gt;% tidyr::expand( date, id_geo_bg_fips = unique(trans_prop$id_geo_bg_fips) ) %&gt;% full_join(trans_prop) %&gt;% group_by(id_geo_bg_fips, date) %&gt;% summarise( sum_log_error = sum(log_error, na.rm = TRUE), sales_total = sum(!is.na(log_error)) ) %&gt;% ungroup() %&gt;% group_by(id_geo_bg_fips) %&gt;% mutate( sum_log_error_7days = rolling_sum_7(sum_log_error), sum_log_error_28days = rolling_sum_28(sum_log_error), roll_bg_trans_total_7days = rolling_sum_7(sales_total), roll_bg_trans_total_28days = rolling_sum_28(sales_total), roll_bg_avg_log_error_7days = sum_log_error_7days / roll_bg_trans_total_7days, roll_bg_avg_log_error_28days = sum_log_error_28days / roll_bg_trans_total_28days, date = date + lubridate::days(1) # to not include the current day in avg ) %&gt;% select( id_geo_bg_fips, date, roll_bg_trans_total_7days, roll_bg_trans_total_28days, roll_bg_avg_log_error_7days, roll_bg_avg_log_error_28days ) %&gt;% mutate( roll_bg_trans_total_7days = ifelse(is.na(roll_bg_trans_total_7days), 0, roll_bg_trans_total_7days), roll_bg_trans_total_28days = ifelse(is.na(roll_bg_trans_total_28days), 0, roll_bg_trans_total_28days), roll_bg_avg_log_error_7days = ifelse(is.nan(roll_bg_avg_log_error_7days), 0, roll_bg_avg_log_error_7days), roll_bg_avg_log_error_28days = ifelse(is.nan(roll_bg_avg_log_error_28days), 0, roll_bg_avg_log_error_28days), roll_bg_avg_log_error_7days = as.numeric(forecast::na.interp(roll_bg_avg_log_error_7days)), roll_bg_avg_log_error_28days = as.numeric(forecast::na.interp(roll_bg_avg_log_error_28days)) ) # by tract ---------------------------------------------------------------- roll_tract &lt;- create_series(min(trans_prop$date) ~ max(trans_prop$date), &#39;daily&#39;, class = &quot;Date&quot;) %&gt;% tidyr::expand( date, id_geo_tract_fips = unique(trans_prop$id_geo_tract_fips) ) %&gt;% full_join(trans_prop) %&gt;% group_by(id_geo_tract_fips, date) %&gt;% summarise( sum_log_error = sum(log_error, na.rm = TRUE), sales_total = sum(!is.na(log_error)) ) %&gt;% ungroup() %&gt;% group_by(id_geo_tract_fips) %&gt;% mutate( sum_log_error_7days = rolling_sum_7(sum_log_error), sum_log_error_28days = rolling_sum_28(sum_log_error), roll_tract_trans_total_7days = rolling_sum_7(sales_total), roll_tract_trans_total_28days = rolling_sum_28(sales_total), roll_tract_avg_log_error_7days = sum_log_error_7days / roll_tract_trans_total_7days, roll_tract_avg_log_error_28days = sum_log_error_28days / roll_tract_trans_total_28days, date = date + lubridate::days(1) # to not include the current day in avg ) %&gt;% select( id_geo_tract_fips, date, roll_tract_trans_total_7days, roll_tract_trans_total_28days, roll_tract_avg_log_error_7days, roll_tract_avg_log_error_28days ) %&gt;% mutate( roll_tract_trans_total_7days = ifelse(is.na(roll_tract_trans_total_7days), 0, roll_tract_trans_total_7days), roll_tract_trans_total_28days = ifelse(is.na(roll_tract_trans_total_28days), 0, roll_tract_trans_total_28days), roll_tract_avg_log_error_7days = ifelse(is.nan(roll_tract_avg_log_error_7days), 0, roll_tract_avg_log_error_7days), roll_tract_avg_log_error_28days = ifelse(is.nan(roll_tract_avg_log_error_28days), 0, roll_tract_avg_log_error_28days), roll_tract_avg_log_error_7days = as.numeric(forecast::na.interp(roll_tract_avg_log_error_7days)), roll_tract_avg_log_error_28days = as.numeric(forecast::na.interp(roll_tract_avg_log_error_28days)) ) prop_geo_ids &lt;- properties_geo %&gt;% select( id_parcel, id_geo_bg_fips, id_geo_tract_fips ) write_feather(roll_bg, &quot;data/external-features/roll_features_blockgroup.feather&quot;) write_feather(roll_tract, &quot;data/external-features/roll_features_tract.feather&quot;) 4.1.2 External Features Breaking from the rules of the actual Kaggle competition, we’re going to add in some external features as an example of bringing in other information 4.1.2.1 American Community Survey The American Community Survey is a great source of demographic and household data. As an example of using this data let’s bring in a few features related to our area of interest. In our example here, we are completely ignoring the margin or error for each feature, given more time investigating the information contained in these fields is most likely worth your while. There are literally thousands you can explore in the ACS. For our example, we are going to stop a little short of that and only add the following Fill this in sub group 1 sub group 2 fill this in library(tidycensus) api_key &lt;- Sys.getenv(&quot;CENSUS_API_KEY&quot;) census_api_key(api_key) acs_var_list &lt;- load_variables(2016, &quot;acs5&quot;, cache = TRUE) acs_bg_vars &lt;- c(&quot;B25034_001E&quot;, &quot;B25034_002E&quot;, &quot;B25034_003E&quot;, &quot;B25034_004E&quot;, &quot;B25034_005E&quot;, &quot;B25034_006E&quot;, &quot;B25034_007E&quot;, &quot;B25034_008E&quot;, &quot;B25034_009E&quot;, &quot;B25034_010E&quot;, &quot;B25034_011E&quot;, &quot;B25076_001E&quot;, &quot;B25077_001E&quot;, &quot;B25078_001E&quot;, &quot;B25056_001E&quot;, &quot;B25002_001E&quot;, &quot;B25002_003E&quot;, &quot;B25001_001E&quot;) acs_bg_home_value &lt;- acs_var_list %&gt;% filter(grepl(&quot;B25075_&quot;, x = name)) acs_bg_home_value_vars &lt;- acs_bg_home_value$name acs_bg_vars &lt;- c(acs_bg_vars, acs_bg_home_value_vars) acs_bg_data &lt;- get_acs( geography = &quot;block group&quot;, variables = acs_bg_vars, state = &quot;CA&quot;, county = c(&quot;Los Angeles&quot;, &quot;Orange&quot;, &quot;Ventura&quot;), output = &quot;wide&quot;, geometry = FALSE, keep_geo_vars = TRUE ) acs_bg_data1 &lt;- acs_bg_data %&gt;% select( id_geo_bg_fips = GEOID, acs_str_yr_total = B25034_001E, acs_str_yr_2014_later = B25034_002E, acs_str_yr_2010_2013 = B25034_003E, acs_str_yr_2000_2009 = B25034_004E, acs_str_yr_1990_1999 = B25034_005E, acs_str_yr_1980_1989 = B25034_006E, acs_str_yr_1970_1979 = B25034_007E, acs_str_yr_1960_1969 = B25034_008E, acs_str_yr_1950_1959 = B25034_009E, acs_str_yr_1940_1949 = B25034_010E, acs_str_yr_1939_earlier = B25034_011E, acs_home_value_lwr = B25076_001E, acs_home_value_med = B25077_001E, acs_home_value_upr = B25078_001E, acs_num_of_renters_total = B25056_001E, acs_num_of_house_units = B25001_001E, acs_occ_status_total = B25002_001E, acs_occ_status_vacant = B25002_003E, acs_home_value_cnt_total = B25075_001E, acs_home_value_cnt_less_10k = B25075_002E, acs_home_value_cnt_10k_15k = B25075_003E, acs_home_value_cnt_15k_20k = B25075_004E, acs_home_value_cnt_20k_25k = B25075_005E, acs_home_value_cnt_25k_30k = B25075_006E, acs_home_value_cnt_30k_35k = B25075_007E, acs_home_value_cnt_35k_40k = B25075_008E, acs_home_value_cnt_40k_50k = B25075_009E, acs_home_value_cnt_50k_60k = B25075_010E, acs_home_value_cnt_60k_70k = B25075_011E, acs_home_value_cnt_70k_80k = B25075_012E, acs_home_value_cnt_80k_90k = B25075_013E, acs_home_value_cnt_90k_100k = B25075_014E, acs_home_value_cnt_100k_125k = B25075_015E, acs_home_value_cnt_125k_150k = B25075_016E, acs_home_value_cnt_150k_175k = B25075_017E, acs_home_value_cnt_175k_200k = B25075_018E, acs_home_value_cnt_200k_250k = B25075_019E, acs_home_value_cnt_250k_300k = B25075_020E, acs_home_value_cnt_300k_400k = B25075_021E, acs_home_value_cnt_400k_500k = B25075_022E, acs_home_value_cnt_500k_750k = B25075_023E, acs_home_value_cnt_750k_1000k = B25075_024E, acs_home_value_cnt_1000k_1500k = B25075_025E, acs_home_value_cnt_1500k_2000k = B25075_026E, acs_home_value_cnt_2000k_more = B25075_027E ) %&gt;% mutate_at( vars(starts_with(&quot;acs_home_value_cnt&quot;)), function(x) round(x / .$acs_home_value_cnt_total, digits = 5) ) %&gt;% mutate_at( vars(starts_with(&quot;acs_str_yr&quot;)), function(x) round(x / .$acs_str_yr_total, digits = 5) ) %&gt;% mutate( acs_per_renters = round(acs_num_of_renters_total / acs_num_of_house_units, digits = 5), acs_per_vacant = round(acs_occ_status_vacant / acs_occ_status_total, digits = 5) ) %&gt;% select( -acs_occ_status_total, -acs_home_value_cnt_total, -acs_str_yr_total ) acs_features &lt;- properties_geo %&gt;% select( id_parcel, id_geo_bg_fips ) %&gt;% left_join(acs_bg_data1, by = &quot;id_geo_bg_fips&quot;) %&gt;% select(-id_geo_bg_fips) properties &lt;- properties %&gt;% left_join(acs_features, by = &quot;id_parcel&quot;) 4.1.2.2 Economic Indicators The value of a home is not only influenced by itself and its neighbors, but also larger economic trends. To help account for this in our model we are going to add in the following economic indicators 30-Year Fixed Rate Mortgage Average in the United States S&amp;P/Case-Shiller CA-Los Angeles Home Price Index Unemployment Rate in Los Angeles County, CA library(alfred) # 30-Year Fixed Rate Mortgage Average in the United States (weekly) mort30 &lt;- get_fred_series(&quot;MORTGAGE30US&quot;) %&gt;% mutate( date_month = floor_date(date, unit = &quot;month&quot;), date_week = floor_date(date, unit = &quot;week&quot;) ) # S&amp;P/Case-Shiller CA-Los Angeles Home Price Index (monthly) spcs &lt;- get_fred_series(&quot;LXXRNSA&quot;) # Unemployment Rate in Los Angeles County, CA (monthly) unemployment &lt;- get_fred_series(&quot;CALOSA7URN&quot;) econ_features &lt;- create_series(min(mort30$date) ~ max(mort30$date), &#39;daily&#39;, class = &quot;Date&quot;) %&gt;% mutate(date_week = floor_date(date, unit = &quot;week&quot;)) %&gt;% left_join(mort30, by = c(&quot;date_week&quot; = &quot;date_week&quot;)) %&gt;% left_join(spcs, by = c(&quot;date_month&quot; = &quot;date&quot;)) %&gt;% left_join(unemployment, by = c(&quot;date_month&quot; = &quot;date&quot;)) %&gt;% select( date = date.x, econ_mort_30 = MORTGAGE30US, econ_case_shiller = LXXRNSA, econ_unemployment = CALOSA7URN ) %&gt;% filter( date &gt;= date(&quot;2015-12-01&quot;), date &lt;= date(&quot;2018-01-01&quot;) ) write_feather(econ_features, &quot;data/external-features/econ_features.feather&quot;) Ok, so a check in on where we are. We currently have 5 data frames of interest, our old friends properties which now contains new features from our bg_avg_features neighborhood data and the acs_features which contain a few indicators from the American Community Survey and trans which contains our response variable log_error as well as the transaction date and a few date based features. The other 3 data frames we have are seperated from the properties and trans data currently because they contain features that have different values for each day and depending on what our transaction dates are for the partiuclar set of observations we will use filter those values down and join them at the time of training. 4.2 Handling Missing Data There are many ways to handle missing data from simple mean or median imputation to more complex methods such as knn or multiple imputation or even constructing other predictive models for predicting missing features. We will not explore that topic in depth here and use a somewhat simple approach that takes advantage of the spatial relationships of our data. We will use median (or modal for nominal features) imputation but instead of doing global median values for all observations, we are going to break our observations into subspaces based on zoning_landuse, area_lot, and increasingly larger neigborhood windows. The reasoning behind this choice is that the values for many of the other features can vary widely across these categories. For example it wouldn’t make sense to include the tax_building values for mobile homes if we are imputing the tax_building value of a commercial office building. The is true for area_lot the tax burden on a large commercial office will be larger then the one of a smaller office. So we will look at increaingly larger neighborhoods of zoning_landuse and (a discretized version of) area_lot combinations, if there are any none NA observations of that combination in the missing observations block group, fill its values with the block group median (mode), if there are no non-missing observations with that combination in that block group, then look at the tract level, if there are none at the tract look at the county level, and finally if there are no non-missing observations in the County that the parcel belongs to, an unlikely event, then use the “global” values to impute. To do this we first need to impute the values for zoning_landuse and area_lot. For this we will just use the increasing neighborhood search for zoning_landuse and then use the increasing neighborhood search broken down by zoning_landuse to fill in area_lot For some reason their are no built in functions for calculating the mode. Make a simple helper function to do so # simple helper function to find the mode fct_mode &lt;- function(f) { f_no_na &lt;- na.omit(f) fct_tab &lt;- table(f_no_na) # if everything NA return NA if (length(fct_tab) == 0) return(NA) modal_fct &lt;- names(fct_tab)[which(fct_tab == max(fct_tab))] modal_fct &lt;- modal_fct[1] # in case of ties, go with first one modal_fct } Some parcels have no information at all including geographic ids. Randomly assign them a id_geo_bg_fips value based block group frequency parcels_no_info &lt;- properties %&gt;% filter( is.na(id_geo_bg_fips) ) %&gt;% select(id_parcel) bg_probs &lt;- table(properties$id_geo_bg_fips) %&gt;% as.data.frame() bg_assignments &lt;- sample(bg_probs$Var1, size = nrow(parcels_no_info), replace = TRUE, prob = bg_probs$Freq) bg_assignments &lt;- as.character(bg_assignments) parcels_no_info_row_id &lt;- properties$id_parcel %in% parcels_no_info$id_parcel properties[parcels_no_info_row_id , &quot;id_geo_bg_fips&quot;] &lt;- bg_assignments # fill in the missing tract and county based on bg properties &lt;- properties %&gt;% group_by(id_geo_bg_fips) %&gt;% mutate( id_geo_tract_fips = fct_mode(id_geo_tract_fips) ) %&gt;% ungroup() %&gt;% group_by(id_geo_tract_fips) %&gt;% mutate( id_geo_county_fips = fct_mode(id_geo_county_fips) ) %&gt;% ungroup() Now that all observations have at least geo id values we can impute zoning_landuse using the increasing neighborhood search properties &lt;- properties %&gt;% group_by(id_geo_bg_fips) %&gt;% mutate( zoning_landuse = replace_na(zoning_landuse, fct_mode(zoning_landuse)) ) %&gt;% ungroup() %&gt;% group_by(id_geo_tract_fips) %&gt;% mutate( zoning_landuse = replace_na(zoning_landuse, fct_mode(zoning_landuse)) ) %&gt;% ungroup() %&gt;% group_by(id_geo_county_fips) %&gt;% mutate( zoning_landuse = replace_na(zoning_landuse, fct_mode(zoning_landuse)) ) %&gt;% ungroup() Now for area_lot properties &lt;- properties %&gt;% group_by( id_geo_bg_fips, zoning_landuse ) %&gt;% mutate( area_lot = replace_na(area_lot, median(area_lot, na.rm = TRUE)) ) %&gt;% ungroup() %&gt;% group_by( id_geo_tract_fips, zoning_landuse ) %&gt;% mutate( area_lot = replace_na(area_lot, median(area_lot, na.rm = TRUE)) ) %&gt;% ungroup() %&gt;% group_by( id_geo_county_fips, zoning_landuse ) %&gt;% mutate( area_lot = replace_na(area_lot, median(area_lot, na.rm = TRUE)) ) %&gt;% ungroup() OK, now for the rest of them. Becasue we used median() to fill in area_lot the quantile() are not unique, so add a little bit of noise area_lot_jitter and base the breaks on those. # impute all other features based on neighborhood, zoning_landuse, and area_lot properties &lt;- properties %&gt;% mutate( area_lot_jitter = area_lot + runif(n = n(), min = -1, max = 1), area_lot_quantile = cut(area_lot_jitter, breaks = quantile(area_lot_jitter, probs = seq(0, 1, 0.1), na.rm = TRUE)) ) %&gt;% group_by( id_geo_bg_fips, zoning_landuse, area_lot_quantile ) %&gt;% mutate_if( is.numeric, .funs = function(x) replace_na(x, median(x, na.rm = TRUE)) ) %&gt;% mutate_if( is.factor, .funs = function(x) replace_na(x, fct_mode(x)) ) %&gt;% ungroup() %&gt;% group_by( id_geo_tract_fips, zoning_landuse, area_lot_quantile ) %&gt;% mutate_if( is.numeric, .funs = function(x) replace_na(x, median(x, na.rm = TRUE)) ) %&gt;% mutate_if( is.factor, .funs = function(x) replace_na(x, fct_mode(x)) ) %&gt;% ungroup() %&gt;% group_by( id_geo_county_fips, zoning_landuse, area_lot_quantile ) %&gt;% mutate_if( is.numeric, .funs = function(x) replace_na(x, median(x, na.rm = TRUE)) ) %&gt;% mutate_if( is.factor, .funs = function(x) replace_na(x, fct_mode(x)) ) %&gt;% ungroup() %&gt;% group_by( zoning_landuse, area_lot_quantile ) %&gt;% mutate_if( is.numeric, .funs = function(x) replace_na(x, median(x, na.rm = TRUE)) ) %&gt;% mutate_if( is.factor, .funs = function(x) replace_na(x, fct_mode(x)) ) %&gt;% ungroup() At this point we now have all the original features we are going to use and have filled in all missing values. The next step is to transform our features, create interaction features, and then move unto feature selection. 4.3 Feature Transformation Combine all of our data and remove a handful of features that we aren’t going to use # have to remove id_geo after joins because of time features d &lt;- trans %&gt;% left_join(properties, by = &quot;id_parcel&quot;) %&gt;% left_join(econ_features, by = &quot;date&quot;) %&gt;% left_join(roll_bg, by = c(&quot;id_geo_bg_fips&quot;, &quot;date&quot;)) %&gt;% left_join(roll_tract, by = c(&quot;id_geo_tract_fips&quot;, &quot;date&quot;)) %&gt;% select( -id_parcel, -abs_log_error, -week, -region_city, -region_zip, -area_lot_jitter, -area_lot_quantile, -lat, # we have other lat/lon features -lon, -starts_with(&quot;id_geo&quot;) ) %&gt;% mutate( date = as.numeric(date), year = factor(year, levels = sort(unique(year)), ordered = TRUE), month_year = factor( as.character(month_year), levels = as.character(unique(sort(month_year))), ordered = TRUE), str_quality = factor(str_quality, levels = 12:1, ordered = TRUE) ) Here we are going to use the fantastic package recipes to handle all of our feature transformations library(recipes) rec &lt;- recipe(d) %&gt;% add_role(log_error, new_role = &#39;outcome&#39;) %&gt;% add_role(-log_error, new_role = &#39;predictor&#39;) %&gt;% step_meanimpute(starts_with(&quot;roll_&quot;)) %&gt;% step_zv(all_numeric()) %&gt;% step_BoxCox( starts_with(&quot;num_&quot;), starts_with(&quot;area_&quot;), starts_with(&quot;tax_&quot;), starts_with(&quot;bg_avg_num_&quot;), starts_with(&quot;bg_avg_area_&quot;), starts_with(&quot;bg_avg_tax_&quot;) ) %&gt;% step_dummy(all_nominal(), one_hot = TRUE) %&gt;% step_interact(~starts_with(&quot;tax_&quot;):starts_with(&quot;area_&quot;)) %&gt;% step_center(all_numeric(), -all_outcomes()) %&gt;% step_scale(all_numeric(), -all_outcomes()) %&gt;% step_pca(starts_with(&quot;acs_home_value_cnt&quot;), prefix = &quot;acs_home_value_cnt_PC&quot;) %&gt;% step_zv(all_numeric()) rec_prepped &lt;- prep(rec, training = d) "],
["feature-selection.html", "Chapter 5 Feature Selection", " Chapter 5 Feature Selection Now that we have our data in something Based on the data frame we created in d and the transformation recipe we made we are going to do some initital analysis on which features we want to keep or drop. the xgboost package provides a function, xgb.importance() that gives a summary of how important each feature was in a model estimated by xgb.train() To have a little more robustness in our selection, we will use v fold cross validation to get mutliple samples from d and investigate the importance of the features across all samples. library(broom) library(purrr) library(xgboost) importance_results &lt;- function(splits) { x &lt;- bake(rec_prepped, newdata = analysis(splits)) y &lt;- x$log_error d &lt;- model.matrix(log_error ~., data = x) d &lt;- xgb.DMatrix(d, label = y) mdl &lt;- xgb.train(data = d, label = y, nrounds = 1000, nthread = 4) print(summary(mdl)) mdl_importance &lt;- as.data.frame(xgb.importance(model = mdl)) mdl_importance } library(rsample) resamples &lt;- vfold_cv(d, v = 10, repeats = 5) resamples$results &lt;- map(resamples$splits, importance_results) importance_df &lt;- bind_rows(resamples$results) feature_avg &lt;- importance_df %&gt;% group_by(Feature) %&gt;% summarise( mean = mean(Gain), sd = sd(Gain), n = n() ) feature_avg %&gt;% ggplot(aes(x = forcats::fct_reorder(Feature, mean), y = mean)) + geom_hline(aes(yintercept = 0.001), colour = &quot;red&quot;, size = 1, alpha = 0.5) + geom_point(size = 1) + geom_errorbar(aes(ymin = mean - sd * 2, ymax = mean + sd * 2)) + coord_flip() + theme_bw() + theme( axis.text=element_text(size = 6) ) + labs( x = &quot;Feature&quot;, y = &quot;Mean Gain&quot; ) Figure 5.1: Mean Feature Importance Based on Cross Validation Using Basic XGBoost Model To reduce the complexity and computation time our of modeling, we are going to remove the feature that consistantly did not provide much value by cutting off the number of features we’ll use at a mean gain at 0.001 (red line). features_to_use &lt;- feature_avg %&gt;% filter(mean &gt;= 0.001) %&gt;% .$Feature "],
["modeling.html", "Chapter 6 Modeling 6.1 Make Predictions with Tuned Parameters", " Chapter 6 Modeling For our first pass a submission, we are going to use the XGBoost model. This model has seen much success in Kaggle competition due to its flexiblity and range of modeling tasks it can be applied to. 6.0.1 XGBoost In gerenal XGBoost works like this… ## Warning: package &#39;recipes&#39; was built under R version 3.4.4 ## Loading required package: broom ## ## Attaching package: &#39;recipes&#39; ## The following object is masked from &#39;package:stringr&#39;: ## ## fixed ## The following object is masked from &#39;package:stats&#39;: ## ## step As a reminder our recipe for our transformations are stored in rec rec ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 98 ## ## Operations: ## ## Mean Imputation for starts_with(&quot;roll_&quot;) ## Zero variance filter on all_numeric() ## Box-Cox transformation on 6 items ## Dummy variables from all_nominal() ## Interactions with starts_with(&quot;tax_&quot;):starts_with(&quot;area_&quot;) ## Centering for all_numeric(), -all_outcomes() ## Scaling for all_numeric(), -all_outcomes() ## PCA extraction with starts_with(&quot;acs_home_value_cnt&quot;) ## Zero variance filter on all_numeric() and the data we have is in d # some kind of d summary 6.0.2 creating our scoring function # create scoring function ------------------------------------------------- xgboost_regress_score &lt;- function(train_df, target_var, params, eval_df, ...) { X_train &lt;- train_df %&gt;% select(features_to_use) %&gt;% as.matrix() y_train &lt;- train_df[[target_var]] xgb_train_data &lt;- xgb.DMatrix(X_train, label = y_train) X_eval &lt;- eval_df %&gt;% select(features_to_use) %&gt;% as.matrix() y_eval &lt;- eval_df[[target_var]] xgb_eval_data &lt;- xgb.DMatrix(X_eval, label = y_eval) model &lt;- xgb.train(params = params, data = xgb_train_data, watchlist = list(train = xgb_train_data, eval = xgb_eval_data), objective = &#39;reg:linear&#39;, verbose = FALSE, nthread = 20, ...) preds &lt;- predict(model, xgb_eval_data) list(mae = MAE(preds, y_eval)) } Make the parameter set that we are going to search over library(ParamHelpers) ## Warning: package &#39;ParamHelpers&#39; was built under R version 3.4.4 # make parameter set ------------------------------------------------------ xgboost_random_params &lt;- makeParamSet( makeIntegerParam(&#39;max_depth&#39;, lower = 1, upper = 15), makeNumericParam(&#39;eta&#39;, lower = 0.01, upper = 0.1), makeNumericParam(&#39;gamma&#39;, lower = 0, upper = 5), makeIntegerParam(&#39;min_child_weight&#39;, lower = 1, upper = 100), makeNumericParam(&#39;subsample&#39;, lower = 0.25, upper = 0.9), makeNumericParam(&#39;colsample_bytree&#39;, lower = 0.25, upper = 0.9) ) set up our cv # create cv resampling ---------------------------------------------------- resamples &lt;- vfold_cv(d, v = 5) Use Ranger model for a progressive zoom into parameter space for tuning library(MLmetrics) library(xgboost) library(tidytune) # perform surrogate search over parameters -------------------------------- n &lt;- c(10, 5, 3, 2) n_candidates &lt;- c(0, 10, 100, 1000) search_results &lt;- surrogate_search( resamples = resamples, recipe = rec, param_set = xgboost_random_params, n = n, scoring_func = xgboost_regress_score, nrounds = 1000, early_stopping_rounds = 20, eval_metric = &#39;mae&#39;, input = NULL, surrogate_target = &#39;mae&#39;, n_candidates = n_candidates, top_n = 5 ) search_summary &lt;- search_results %&gt;% group_by_at(getParamIds(xgboost_random_params)) %&gt;% summarise(mae = mean(mae)) %&gt;% arrange(mae) search_results %&gt;% group_by_at( c(&quot;surrogate_run&quot;, &quot;surrogate_iteration&quot;, &quot;param_id&quot;, getParamIds(xgboost_random_params) ) ) %&gt;% summarise(mae = mean(mae)) %&gt;% ungroup() %&gt;% mutate(surrogate_run = factor(surrogate_run)) %&gt;% arrange( surrogate_run, surrogate_iteration ) %&gt;% mutate( iteration = row_number() ) %&gt;% ggplot(aes(x = iteration, y = mae)) + geom_smooth(alpha = 0.2, size = 0.8, colour = &quot;grey&quot;) + geom_point(aes(col = surrogate_run)) + theme_bw() + labs( y = &quot;MAE&quot;, x = &quot;iteration&quot;, col = &quot;Surrogate Run&quot; ) Figure 6.1: Mean Absolute Error Progressively Decreasing with Each Surrogate Run 6.1 Make Predictions with Tuned Parameters tuned_params &lt;- search_summary %&gt;% ungroup() %&gt;% filter(mae == min(mae)) %&gt;% select(getParamIds(xgboost_random_params)) %&gt;% as.list() tuned_params ## $max_depth ## [1] 5 ## ## $eta ## [1] 0.07531416 ## ## $gamma ## [1] 2.009971 ## ## $min_child_weight ## [1] 100 ## ## $subsample ## [1] 0.8821157 ## ## $colsample_bytree ## [1] 0.5284899 Train the model using the tuned parameters d_prepped &lt;- prep(rec) train_df &lt;- bake(d_prepped, newdata = d) x_train &lt;- train_df %&gt;% select(features_to_use) %&gt;% as.matrix() y_train &lt;- train_df$log_error xgb_train_data &lt;- xgb.DMatrix(x_train, label = y_train) model &lt;- xgb.train(params = tuned_params, data = xgb_train_data, objective = &#39;reg:linear&#39;, verbose = FALSE, nthread = 4, nrounds = 1000) Now we need to make our predictions. We’ll make a helper function predict_date() to do this. predict_date &lt;- function(parcel_id, predict_date, mdl) { d_predict_ids &lt;- properties %&gt;% filter(id_parcel %in% parcel_id) %&gt;% crossing(date = predict_date) d_predict &lt;- d_predict_ids %&gt;% mutate( year = year(date), month_year = make_date(year(date), month(date)), month = month(date, label = TRUE), week = floor_date(date, unit = &quot;week&quot;), week_of_year = week(date), week_since_start = (min(date) %--% date %/% dweeks()) + 1, wday = wday(date, label = TRUE), day_of_month = day(date) ) %&gt;% left_join(econ_features, by = &quot;date&quot;) %&gt;% left_join(roll_bg, by = c(&quot;id_geo_bg_fips&quot;, &quot;date&quot;)) %&gt;% left_join(roll_tract, by = c(&quot;id_geo_tract_fips&quot;, &quot;date&quot;)) %&gt;% select( -id_parcel, -week, -region_city, -region_zip, -area_lot_jitter, -area_lot_quantile, -lat, # we have other lat/lon features -lon, -starts_with(&quot;id_geo&quot;) ) %&gt;% mutate( date = as.numeric(date), year = factor(year, levels = sort(unique(year)), ordered = TRUE), month_year = factor( as.character(month_year), levels = as.character(unique(sort(month_year))), ordered = TRUE), str_quality = factor(str_quality, levels = 12:1, ordered = TRUE) ) eval_df &lt;- bake(d_prepped, newdata = d_predict) x_eval &lt;- eval_df %&gt;% select(features_to_use) %&gt;% as.matrix() preds &lt;- predict(mdl, x_eval) properties_predict &lt;- d_predict_ids %&gt;% select( id_parcel, date ) %&gt;% mutate( pred = preds ) %&gt;% spread(date, pred) names(properties_predict) &lt;- c(&quot;ParcelId&quot;, &quot;201610&quot;,&quot;201611&quot;,&quot;201612&quot;, &quot;201710&quot;,&quot;201711&quot;,&quot;201712&quot;) properties_predict } The submission requires a prediction for Oct-Dec 2016 and Oct-Dec 2017. This means that the prediction is for any day in that month. For our example first submission, we are going to just set the date to the first wednesday in each month. This is completely arbitrary. Another approach would be to make predictions for every day in each month and submit the mean prediction for each month. We’ll save this for later work. # first wednesday in each month predict_dates &lt;- date(c(&quot;2016-10-06&quot;,&quot;2016-11-02&quot;,&quot;2016-12-07&quot;, &quot;2017-10-04&quot;,&quot;2017-11-01&quot;,&quot;2017-12-06&quot;)) # split parcels to chunck our predictions id_parcels &lt;- properties$id_parcel id_parcel_splits &lt;- split(id_parcels, ceiling(seq_along(id_parcels) / 5000)) predict_list &lt;- lapply(id_parcel_splits, function(i) { pred_df &lt;- predict_date( parcel_id = i, predict_date = predict_dates, mdl = model ) }) # they only evaluate to 4 decimcals so round to save space # Convert ParcelId to integer to prevent Sci Notation that causes # issues with submission predict_df &lt;- bind_rows(predict_list) %&gt;% mutate_at(vars(`201610`:`201712`), round, digits = 4) %&gt;% mutate(ParcelId = as.integer(ParcelId)) %&gt;% as.data.frame() write_csv(predict_df, &quot;data/submit01.csv&quot;) This model produced a MAE of 0.0651839 on the public leaderboard, which is honestly not that great, but it is a starting point that we can now start iterarting from. My first thought is perhaps we are overfitting on our training data, we could start exploring how differnt tuning affect actual submissions and not just cross validation based on resampling. Based on this we can tract how our performance changes and start narrow down what what tuning are most performant for this task. Another approach would be to only use "],
["summary.html", "Chapter 7 Summary 7.1 Key Findings 7.2 Weak Points of Analysis", " Chapter 7 Summary 7.1 Key Findings 7.2 Weak Points of Analysis imputation method need to test more models could include more geo related features like interstate density should do a base line comparision "]
]
